[{"body":" Luis Gascó, PhD. Avature, Spain Hermenegildo Fabregat, PhD. Avature, Spain Laura García-Sardiña Avature, Spain Daniel Deniz Cerpa, PhD. Avature, Spain Paula Estrella Avature, Spain Alvaro Rodrigo, PhD. UNED, Spain Rabih Zbib, PhD. Avature, Spain ","categories":["Examples"],"description":"TalentCLEF 2025 is organised by:\n","excerpt":"TalentCLEF 2025 is organised by:\n","ref":"/talentclef/docs/talentclef-2025/people/task_organizers/","tags":["test","sample","docs"],"title":"Task Organizers"},{"body":" We take the legal and ethical implications of using AI in Human Resources very seriously. It is important to note that the data we utilize inherently excludes any personal information, focusing solely on job titles and skills without involving personal/company information or geographic location. This page contains the corpus information for TalentCLEF 2025 tasks. You can access the link to download in datasets page.\nTask A: Multilingual Job Title Matching Corpus Summary: The corpus used for Task A consists of a set of job titles in three languages: English, Spanish and German, from different job domains and professional sectors. These job titles have been collected and processed in order to facilitate the identification and comparison of equivalent titles across languages.\nThe training corpus has been generated using public terminologies, ensuring that the job titles are representative of a wide range of job domains and aligned with standard market terminology.\nOn the other hand, the validation and test corpora have been annotated by domain experts, following well-defined guidelines to ensure consistency and quality of labels. This annotation process, performed with specialized tools, included several stages of quality control to ensure that the labels were accurate and that the annotated titles accurately reflected the relationships between the different languages in a work environment.\nData:\nTraining Set: The training data is provided in a tabular format with three columns:\nfamily_id: The ISCO family id representing the group to which the job identifier belongs. id: An ESCO identifier indicating the origin of the pair’s job titles. jobtitle_1: The first job title in the pair. jobtitle_2: A second job title related to jobtitle_1. Each dataset is provided in separate files for each language involved in the task. The files are named according to the language, with the following format:\ntaskA_training_en.tsv: Contains related job titles in English. taskA_training_es.tsv: Contains related job titles in Spanish. taskA_training_de.tsv: Contains related job titles in German. An example of the content of these files is shown below:\nfamily_id id jobtitle_1 jobtitle_2 http://data.europa.eu/esco/isco/C2512 http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 desarrollador de software desarrolladora de soluciones http://data.europa.eu/esco/isco/C2512 http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 desarrollador de software ingeniera de aplicaciones http://data.europa.eu/esco/isco/C2512 http://data.europa.eu/esco/occupation/d0aa0792-4345-474b-9365-686cf4869d2e diseñador de software ingeniero de software Validation Set: The validation set is structured into three diferent files: queries, corpus elements and q_rels, and is provided separately for each language.\nQueries: The queries file contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: The corpus elements file contains the following fields:\nc_id: A unique identifier for each corpus element. jobtitle: The job title present in the corpus. qrels: This file defines the relationship between the query and the corpus elements. It does not include a column header, but one is shown here for illustrative purposes.\nq_id: The identifier of the query. iter: A reserved field (always 0). c_id: The identifier of the corresponding corpus element. relevance: A binary score (0 or 1) indicating the relevance of the corpus element to the query, where 1 signifies relevant and 0 non-relevant. We will provide validation set in english, spanish, german and chinese.\nExample of the content of these files for english:\nqueries q_id jobtitle 1 3d animator corpus_elements c_id jobtitle 1 animation artist 2 3d character animator 3 character technical director 4 character designer 5 animation lead 6 3d generalist 7 animator 8 character rigger 9 character animator q_rels q_id iter c_id relevance 1 0 2 1 1 0 3 1 1 0 4 1 1 0 5 1 1 0 6 1 1 0 7 1 1 0 8 1 1 0 9 1 Test Set: The test set is provided with two files: queries and corpus elements, and is provided separately for each language. For every language pair, participants need to generate a TREC Run File that adheres to the format specified in the submission format section. The data structure is similar to the one from the validation files.\nQueries: Contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: Contains:\nq_id: A unique identifier for each corpus element. jobtitle: The job title from the corpus element. Task B: Job Title-Based Skill Prediction Corpus Summary:\nThe dataset is designed to support job title-based skill prediction tasks in English across various job domains and professional sectors. It includes job titles and associated skills collected and processed to facilitate the training of models to solve this task.\nAs with Task A, the training data uses public terminologies to represent a broad spectrum of job domains, while the validation and test sets are annotated by domain experts. This expert annotation follows strict guidelines and quality control measures to ensure consistent labeling and accurate representation of job-title-to-skill relationships.\nData:\nTraining Set: For generating the training data for Task B, the information available in ESCO has been used. We have prepared the training data in three separate files: job2skill.tsv, jobid2terms.json and skillid2terms.json.\njob2skill.tsv: This file has been curated to include the most representative skills for each job title in ESCO. A filtering process has been applied to the number of skills per job title to avoid outliers. This file contains three columns:\njob_id: ESCO identifier for the job position. skill_id: ESCO identifier for the skill. rel_type:Indicator specifying whether the skill_id is essential or optional for a specific job_id. It can have the value “essential” or “optional.” An example of the content of this file is shown below:\njob_id skill_id rel_type http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/8b94aa1e-89c9-459d-b3b4-1dfab8dec2df essential http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/f84a433f-34f1-4083-b0a3-24802623509c essential http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/fd33c66c-70c4-40e6-b87c-5495bd3bf26e optional jobid2terms.json: This JSON file contains job_id identifiers used in the training set for Task A as keys, and a list of valid lexical variants for each identifier as values.\n{ \"http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1\": [ \"application developer\", \"application programmer\", \"applications engineer\", \"application software developer\", \"battery software developer\", \"developer of software\", \"programmer\", \"soft developer\", \"software developer\", \"software developers\", \"software engineer\", \"software specialist\", \"solutions developer\" ] ... } skillid2terms.json: This JSON file contains skill_id identifiers as keys, and a list of valid lexical variants for each identifier as values.\n{ \"http://data.europa.eu/esco/skill/f84a433f-34f1-4083-b0a3-24802623509c\": [ \"web services\", \"web services systems\" ], \"http://data.europa.eu/esco/skill/fd33c66c-70c4-40e6-b87c-5495bd3bf26e\": [ \"design user interface\" ] } Validation Set: The validation set is divided into three diferent files: queries, corpus elements and q_rels:\nQueries: Contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: Contains:\nc_id: A unique identifier for each corpus element. esco_uri: The ESCO URIs associated to c_id. skill_aliases: The list aliases of the ESCO skill q_rels: This file maps the relationship between the query and the corpus elements:\nq_id: The identifier of the query. iter: A reserved field (always 0). c_id: The identifier of the corresponding corpus element. relevance: A binary score (0 or 1) indicating the relevance of the corpus element to the query, where 1 signifies relevant and 0 non-relevant. Example of the content of these files:\nqueries q_id jobtitle dev_qb_jt_1 corporate governance analyst corpus_elements c_id esco_uri skill_aliases dev_cb_sk_1 http://data.europa.eu/esco/skill/1c460d2d-90c6-4fc9-ad49-febb6e15605a [‘pricing plans’, ‘price strategies’, ‘pricing tactics’, ‘pricing strategies’, ‘pricing strategy’] dev_cb_sk_2 http://data.europa.eu/esco/skill/301a6581-e983-4bb6-8b31-b3ee2cbc2392 [‘putting out fires’, …, ‘fires putting out’] dev_cb_sk_3 http://data.europa.eu/esco/skill/a4881e54-6055-4e61-855a-0a56ced7cfa3 [‘online assessment’, ‘analysis of web strategy’, ‘web presence assessment’, ‘web strategy assessment’] dev_cb_sk_4 http://data.europa.eu/esco/skill/efda73b4-5212-40a7-b2f8-d2f754ffdf2b [‘keeping up with trends’, ‘keep pace with trends’, ‘follow trends’, …, ‘keep up with trends’] dev_cb_sk_5 http://data.europa.eu/esco/skill/22a173f5-868c-4d82-87e6-beed500ce070 [‘prepare tax returns form’, …, ‘make tax returns forms ready’, ‘preparing tax returns forms’] dev_cb_sk_6 http://data.europa.eu/esco/skill/97b890ff-acd7-46ad-8d3a-4186f4d42bbf [’tuning procedures’, …, ’tuning skills’, ’tuning techniques’] dev_cb_sk_7 http://data.europa.eu/esco/skill/d5c20065-1d1f-446b-8143-9d1e180c512b [‘iconography methods’, ‘iconography’] q_rels q_id iter c_id relevance dev_qb_jt_1 0 dev_cb_sk_1034 1 dev_qb_jt_1 0 dev_cb_sk_1087 1 dev_qb_jt_1 0 dev_cb_sk_1088 1 dev_qb_jt_1 0 dev_cb_sk_1099 1 dev_qb_jt_1 0 dev_cb_sk_1104 1 dev_qb_jt_1 0 dev_cb_sk_1107 1 dev_qb_jt_1 0 dev_cb_sk_1110 1 dev_qb_jt_1 0 dev_cb_sk_1112 1 Test Set: The test set consists of two files: queries and corpus elements. Participants are required to produce a TREC Run File adhering to the structure outlined in the submission format section. The test set includes a background set in the queries.\nQueries: Contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: Contains:\nc_id: A unique identifier for each corpus element. esco_uri: The ESCO URIs associated to c_id. skill_aliases: The list aliases of the ESCO skill ","categories":["Examples"],"description":"","excerpt":" We take the legal and ethical implications of …","ref":"/talentclef/docs/talentclef-2025/data/description_corpus/","tags":["test","sample","docs"],"title":"Description of the Corpus"},{"body":" In today’s rapidly changing socio-technological landscape, industries and workplaces are transforming quickly. Technological advancements, such as task automation and Artificial Intelligence (AI), are reshaping the labor market by creating new roles that demand specialized skills, often difficult to source. The rise of remote hiring, fueled by technological innovation, has expanded the labor market to a global and multilingual scale. Simultaneously, social progress is narrowing ethnic and gender disparities within companies, fostering more inclusive workplaces.\nSimultaneously, there has been rapid progress in the development and deployment of language-based systems, driven in part by the creation of the Large Language Models (LLMs). These advances are revolutioning the use of tecnology in Human Capital Management (HCM) and Human Resources (HR), enabling the generation of systems able to process large volumens of data and facilitate the identification of the best candidates for specific roles based on their resumee information.\nIntegrating language technologies into HCM significantly enhances key areas. In sourcing and hiring, these tools improve candidate matching by analyzing their skills and experience. During onboarding and training, they create personalized learning materials tailored to individual employee needs. For strategic workforce planning, NLP tools predict market skill trends and future company demands. Additionally, in career development, these technologies monitor employee progress, supporting targeted upskilling and reskilling aligned with both organizational goals and personal aspirations.\nDespite all these benefits, the development and implementation of these systems present challenges such as:\nMultilingualism: The global nature of modern workforces means that companies often need to manage employees and candidates who speak multiple languages. This requires language-based systems to not only understand and process various languages accurately but also to maintain the context and cultural nuances inherent in each. Developing systems that effectively handle multiple languages is a complex task that involves significant computational resources and sophisticated NLP techniques.\nFair models: Ensuring fairness and reducing bias is a critical challenge in HCM. These systems can inadvertently perpetuate existing biases present in the data they are trained on, which can affect hiring decisions, employee evaluations, and promotions. Creating fair and unbiased models requires careful data curation, continuous monitoring, and algorithmic adjustments to mitigate biases related to gender, ethnicity, and other social factors.\nCross-Industry Adaptability: NLP systems must be flexible enough to align with the unique requirements, standards, and practices of each sector, from healthcare to technology to retail, ensuring they are effective and relevant in various contexts.\nThe first edition of TalentCLEF aims to develop and evaluate models designed to facilitate three essential tasks:\nFinding/ranking candidates for job positions based on their experience and professional skills. Implementing upskilling and reskilling strategies that promote the coninuous development of workers Detecting emerging skills and skills gaps of importance in organizations. ","categories":"","description":"","excerpt":" In today’s rapidly changing socio-technological …","ref":"/talentclef/docs/talentclef-2025/motivation/","tags":"","title":"Motivation"},{"body":" The TalentCLEF 2025 task is structured into two independent sub-tasks, each taking account a particular use case scenario:\nTask A: Multilingual Job Title Matching Goal: Develop systems that can identify and rank job titles most similar to a given job title. For each job title in a provided test set, participants must generate a ranked list of similar job titles from a specified knowledge base.\nMultilingual: Participants are required to develop systems adapted to English, Spanish, German, and optionally, Chinese.\nData: More information about data will be shown in Data section, but essentially we will provide:\nTraining Set: A collection of 15,000 pairs of related job titles per language (English, Spanish, and German), each labeled with a concept identifier, will be provided to facilitate the creation of cross-language training samples. No training data will be provided in Chinese, opening the possibility to use different techniques to improve the performance of the models in this language.\nDevelopment Set: Participants will receive a manually annotated evaluation set of 100 samples per language, consisting of a job title and its list of related job titles. A knowledge base of 2,500 job titles in each task language will also be provided for participants to generate predictions by ranking it.\nTest set: A background set comprising 5,000 job titles will be provided. The evaluation, however, will be conducted on a subset of the background set, that will be a gold standard corpus of 100 job titles in each language annotated with the same methodology as the development set.\nEvaluation: Details about evaluation will be placed in Evaluation page, but the model performance will be evaluated with information retrieval metrics, being the Mean Average Precision (MAP) the official metric of the task, although results will be provided in other metrics such as Mean Reciprocal Rank (MRR) and Precision@K(1,5,10).\nTask B: Job Title-Based Skill Prediction Goal: Develop systems capable of retrieving relevant skills associated with a given job title.\nData: More information about data will be shown in Data section, but essentially we will provide:\nTraining set: A training set of at least 5.000 job titles along with the professional skills required for each position will be provided. This data is sourced from actual job descriptions and semi-automatically curated to ensure high accuracy in the training set.\nDevelopment set: The development set will consist of 200 job titles along with their related skills, normalized to ESCO terminology.\nTest set: The test set comprises a list of 500 job titles. The participants will be required to predict the related skills using the provided gazetteer.\nEvaluation: Details about evaluation will be placed in Evaluation page, but the model performance in this task will be also evaluated with information retrieval metrics, being the Mean Average Precision (MAP) the official metric of the task, although results will be provided in other metrics such as Mean Reciprocal Rank (MRR) and Precision@K(1,5,10).\n","categories":"","description":"","excerpt":" The TalentCLEF 2025 task is structured into two …","ref":"/talentclef/docs/talentclef-2025/task-summary/","tags":"","title":"Task Summary"},{"body":" We take the legal and ethical implications of using AI in Human Resources very seriously. It is important to note that the data we utilize inherently excludes any personal information, focusing solely on job titles and skills without involving personal/company information or geographic location. The data will be hosted on the Zenodo platform under the NLP in HR community, following the file structure outlined below. Each time new data is added, an updated version of the dataset will be published on the platform. Access the Zenodo download page The dataset structure on Zenodo is organized into two *.zip files, TaskA and TaskB, each containing training, validation and test folders to suuport different stages of model development. Until the official release of the full training set, users can access a sample version of the data through the sampleset_TaskA.zip and sampleset_TaskB.zip files.\nTaskA includes language-specific subfolders within the training and validation directories, covering English, Spanish, German, and Chinese job title data. The training folders for TaskA contain language-specific .tsv files for each respective language. Validation folders include three essential files—queries, corpus_elements, and q_rels—for evaluating model relevance to search queries. TaskA’s test folder has queries and corpus_elements files for testing every language considered. Participant can combine queries and corpus elements for the cross-lingual evaluation of the Task. The data can be found in the TaskA.zip file.\n🗜️️ TaskA 📁 training 📁 english 📄 taskA_training_en.tsv 📁 spanish 📄 taskA_training_es.tsv 📁 german 📄 taskA_training_de.tsv 📁 validation 📁 english 📄 queries 📄 corpus_elements 📄 qrels.tsv 📁 spanish 📁 german 📁 chinese 📁 test 📁 english 📄 queries 📄 corpus_elements 📁 spanish 📁 german 📁 chinese TaskB follows a similar structure but without language-specific subfolders, providing general .tsv files for training, validation, and testing. This consistent file organization enables efficient data access and structured updates as new data versions are published. The data can be found in the TaskB.zip file.\n🗜️️ TaskB 📁 training 📄 job2skill.tsv 📄 jobid2terms.json 📄 skillid2terms.json 📁 validation 📄 queries 📄 corpus_elements 📄 q_rels 📁 test 📄 queries 📄 corpus_elements ","categories":["Examples"],"description":"","excerpt":" We take the legal and ethical implications of …","ref":"/talentclef/docs/talentclef-2025/data/datasets/","tags":["test","sample","docs"],"title":"Datasets"},{"body":" Eneko Agirre - Full Professor of the University of the Basque Country UPV/EHU - ACL Fellow David Camacho - Full Professor of the Technical University of Madrid (UPM) Debora Nozza - Assistant Professor of Bocconi University Jens-Joris Decorte - Lead AI Scientist at TechWolf David Graus - Lead Data Scientist at Randstad Group Mesutt Kayaa - Postdoctoral Researcher at Jobindex A/S and IT University Copenhagen Jan Luts - Senior Data Scientist at NTT Data \u0026 ESCO Elena Montiel-Ponsoda - Professor at the Technical University of Madrid (UPM) - AI4Labour project Javier Huertas Tato - Assistant Professor of the Technical University of Madrid (UPM) Patricia Martín Chozas - Postdoctoral Researcher at the Ontology Engineering Group (UPM) - AI4Labour project ","categories":["Examples"],"description":"Scientific committee of the TalentCLEF 2025 Task\n","excerpt":"Scientific committee of the TalentCLEF 2025 Task\n","ref":"/talentclef/docs/talentclef-2025/people/scientific_committee/","tags":["test","sample","docs"],"title":"Scientific Committee"},{"body":" The deadline for submitting your Working Notes is 4th June! Please make sure to submit your Working Notes via EasyChair for CLEF2025. Don’t miss the deadline to ensure your work is included in the official CLEF 2025 proceedings! Here you can find the guidelines to upload your work! Registration for TalentCLEF 2025 is now closed.\n","categories":"","description":"Information about registration procedures","excerpt":"Information about registration procedures","ref":"/talentclef/docs/talentclef-2025/registration/","tags":"","title":"Registration"},{"body":"To support your participation in this shared task, we have compiled a list of additional resources that may be useful for understanding the task better, exploring related work, and utilizing domain-specific models.\n1. Related papers: Gasco, L., Fabregat, H., García-Sardiña, L., Deniz, D., Rodrigo, A., Estrella, P., \u0026 Zbib, R. (2025, April). TalentCLEF at CLEF2025: Skill and Job Title Intelligence for Human Capital Management. In European Conference on Information Retrieval (pp. 479-486). Link Zbib, R., Lacasa, L. A., Retyk, F., Poves, R., Aizpuru, J., Fabregat, H., … \u0026 García-Casademont, E. (2022). Learning Job Titles Similarity from Noisy Skill Labels. arXiv preprint arXiv:2207.00494 Deniz, D., Retyk, F., García-Sardiña, L., Fabregat, H., Gasco, L., \u0026 Zbib, R. (2024). Combined Unsupervised and Contrastive Learning for Multilingual Job Recommendation. Link CEUR Decorte, J. J., Van Hautte, J., Demeester, T., \u0026 Develder, C. (2021). Jobbert: Understanding job titles through skills. arXiv preprint arXiv:2109.09605 Anand, S., Decorte, J. J., \u0026 Lowie, N. (2022). Is it required? ranking the skills required for a job-title. arXiv preprint arXiv:2212.08553 Zhang, M., Van Der Goot, R., \u0026 Plank, B. (2023). ESCOXLM-R: Multilingual taxonomy-driven pre-training for the job market domain. arXiv preprint arXiv:2305.12092 Bhola, A., Halder, K., Prasad, A., \u0026 Kan, M. Y. (2020, December). Retrieving skills from job descriptions: A language model based extreme multi-label classification framework. In Proceedings of the 28th international conference on computational linguistics (pp. 5832-5842). Link Retyk, F., Gasco, L., Carrino, C. P., Deniz, D., \u0026 Zbib, R. (2024). MELO: An Evaluation Benchmark for Multilingual Entity Linking of Occupations. arXiv preprint arXiv:2410.08319. Laosaengpha, N., Tativannarat, T., Rutherford, A., \u0026 Chuangsuwanich, E. (2025). Mitigating Language Bias in Cross-Lingual Job Retrieval: A Recruitment Platform Perspective. arXiv preprint arXiv:2502.03220 Laosaengpha, N., Tativannarat, T., Piansaddhayanon, C., Rutherford, A., \u0026 Chuangsuwanich, E. (2024). Learning Job Title Representation from Job Description Aggregation Network. arXiv preprint arXiv:2406.08055 2. External Resources: ESCOXLM-R Model in Huggingface NESTA Taxonomy ESCO Taxonomy 3. Tutorials: We will publish a series of notebooks covering the fundamentals, including how to work with the data and upload predictions to Codalab.\nNotebook Colab Data Download and Load Task A - Prepare submission file and run evaluation Task A - Development set Baseline generation Task B - Prepare submission file and run evaluation Task A - Test set Baseline Generation Task B - Test set Baseline Generation ","categories":["Examples"],"description":"","excerpt":"To support your participation in this shared task, …","ref":"/talentclef/docs/talentclef-2025/data/additional_resources/","tags":["test","sample","docs"],"title":"Additional resources"},{"body":" The evaluation for TalentCLEF-2025 will be conducted on Codabench. Submissions will be ranked using Mean Average Precision (MAP).\nTalentCLEF Task A - Codabench TalentCLEF Task B - Codabench Evaluation dates Task A: Start Date: 21st April 2025 End Date: 5th May 2025 Task B: Start Date: 28st April 2025 End Date: 5th May 2025 Evaluation Criteria The top-performing teams will be determined based on the following evaluation criteria:\nTask A:\nBest Overall Multilingual Performance – The highest-performing system across English, Spanish, and German, measured as the average Mean Average Precision (MAP) in en-en, es-es, and de-de. Best Cross-Lingual Performance – The best-performing system in cross-lingual scenarios, calculated as the average MAP in en-es and en-de. Best Bias-Controlled Model – The system that minimizes performance differences across job titles in different gender groups. Additionally, there will be a special mention for the best-performing model in Chinese. During the CLEF workshop, certificates will be awarded to the first and second-best systems in each category.\nTask B: The highest-performing system based on MAP.\n","categories":["Examples"],"description":"","excerpt":" The evaluation for TalentCLEF-2025 will be …","ref":"/talentclef/docs/talentclef-2025/evaluation/evaluation_info/","tags":"","title":"Evaluation info"},{"body":" You can find a tutorial on the process of generating submission files and evaluating them in the Tutorials section of the Additional resources page Submissions must adhere to the TREC Run File Format, including column names, with exactly six space-separated columns per line, structured as follows:\n\u003cq_id\u003e Q0 \u003cdoc_id\u003e \u003crank\u003e \u003cscore\u003e \u003crun_tag\u003e Column Descriptions\n\u003cq_id\u003e: The identifier for the query corresponding to the job title being searched. Q0: A constant placeholder (always set to “Q0”). \u003cdoc_id\u003e: The identifier for the retrieved corpus element. \u003crank\u003e: The ranking position of the retrieved document for the query (starting at 1). \u003cscore\u003e: The retrieval score assigned to the document (higher scores indicate greater relevance). \u003crun_tag\u003e: A label identifying the submitted run (e.g., teamA_systemA). Example The submissions must contains the column headers:\nq_id Q0 doc_id rank score tag query1 Q0 ce1 1 0.6910377144813538 teamA_systemA query1 Q0 ce4 2 0.6690284013748169 teamA_systemA query1 Q0 ce5 3 0.662328839302063 teamA_systemA ","categories":["Examples"],"description":"","excerpt":" You can find a tutorial on the process of …","ref":"/talentclef/docs/talentclef-2025/evaluation/submission_format/","tags":["test","sample","docs"],"title":"Submission format"},{"body":" The deadline for submitting your Working Notes is 4th June! Please make sure to submit your Working Notes via EasyChair for CLEF2025. Don’t miss the deadline to ensure your work is included in the official CLEF 2025 proceedings! Here you can find the guidelines to upload your work! Status Date Event Link ✔ 9th September 2024 Website release ✔ 25th October 2024 Sample set release Link ✔ 13th November 2024 Registration opens Link ✔ 20th January 2025 Training data available for Tasks A and B Link ✔ 17th February 2025 Start of Task A with the release of the development data Link ✔ 17th March 2025 Start of Task B with the release of the development data Link ✔ 21st April 2025 Task A Test set release Link ✔ 21st April - 5th10th May 2025 Evaluation period of Task A Codabench Task A ✔ 28st April 2025 Task B Test set release Link ✔ 28st April - 5th10th May 2025 Evaluation period of Task B Codabench Task B ✔ 7th May 202512th May 2025 Publication of Official Results Task A and Task B 30th May 20254th June 2025 Submission of CLEF 2025 Working Notes 4th June 2025 - 27th June Review of Working Notes 27th June 2025 Notification of Acceptance for Participant Papers ","categories":"","description":"Schedule and task deadlines\n","excerpt":"Schedule and task deadlines\n","ref":"/talentclef/docs/talentclef-2025/schedule/","tags":"","title":"Schedule"},{"body":" The evaluation will take place on Codabench, where participants must upload their predictions according to the submission format. The official competition links will be shared soon through this platform and online communication channels.\nTask A - Codabench rules For Task A, predictions for each language pair must be generated in separate .trec files. These files should be compressed into a ZIP file, ensuring that the files are placed directly inside the archive (without any folders), and then uploaded to Codabench. During the development phase, participants can submit as many files as they wish in the platform. During the test phase, a maximum of 6 submissions can be uploaded. Each submission is required to contain at least the results for en-en, es-es and de-de; otherwise, an evaluation error will appear. We also suggest including cross-lingual predictions (en-es, en-de), as well as the Chinese results (zh-zh, en-zh). Uploading rules:\nFile Naming Rules: TREC files must follow the naming convention run_xx-xx_teamname.trec, where xx-xx represents the directionality of the queries and corpus elements. ZIP Compression: The .trec files should be compressed into a flat ZIP file (i.e., the .trec files must be placed directly inside the ZIP, without folders). Task B - Codabench rules During the development phase, participants can submit as many files as they wish in the platform. During the test phase, a maximum of 3 submissions can be uploaded. For local testing, an evaluation script is available in our GitHub repository. You can access it here: Evaluation Script. You can find a tutorial on the process of generating submission files and evaluating them in the Tutorials section of the Additional resources page\nCodabench Tutorial To Be Published\n","categories":["Examples"],"description":"","excerpt":" The evaluation will take place on Codabench, …","ref":"/talentclef/docs/talentclef-2025/evaluation/codabench/","tags":"","title":"Codabench"},{"body":"","categories":"","description":"","excerpt":"","ref":"/talentclef/docs/talentclef-2025/data/","tags":"","title":"Data"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/talentclef/docs/talentclef-2025/evaluation/","tags":"","title":"Evaluation"},{"body":" Info. References to the publications generated in the task will be listed here after the completion of the task. ","categories":"","description":"","excerpt":" Info. References to the publications generated in …","ref":"/talentclef/docs/talentclef-2025/publications/","tags":"","title":"Publications"},{"body":"","categories":"","description":"","excerpt":"","ref":"/talentclef/docs/talentclef-2025/people/","tags":"","title":"People"},{"body":"If you have any concerns related to data or tasks, please don’t hesitate to reach out to us. You can contact us via email by replacing [at] with @ and [dot] with . in the following addresses:\nEmail:\nluis[dot]gasco[at]avature[dot]net hermenegildo[dot]fabregat[at]avature[dot]net ","categories":"","description":"","excerpt":"If you have any concerns related to data or tasks, …","ref":"/talentclef/docs/talentclef-2025/contact/","tags":"","title":"Contact"},{"body":" TalentCLEF Workshop @ CLEF 2025 TalentCLEF 2025 workshop will be held as part of the CLEF 2025 conference (Conference and Labs of the Evaluation Forum), scheduled for September in Madrid, Spain.\nThe TalentCLEF evaluation Lab will be a one-day event that will include several activities. There will be oral presentations of the best challenge solutions, keynote talks, a poster session for participants and a panel discussion. In order to boost participation, an awards ceremony will be held where diplomas will be awarded to the best performing teams. Below is a tentative schedule of the workshop activities.\nCLEF focuses on evaluating the effectiveness of information retrieval systems, such as search engines, text and multimedia retrieval systems, and other types of digital information systems.\nParticipants in the TalentCLEF workshop are required to submit a short paper detailing the systems they have developed for the task. Accepted submissions will be published in the CLEF 2025 CEUR proceedings.\nAt least one author from each accepted paper must register for the CLEF conference and present their system in a poster format. Additionally, outstanding participants, selected by the program committee, will have the opportunity to deliver an oral presentation of their work.\n","categories":"","description":"","excerpt":" TalentCLEF Workshop @ CLEF 2025 TalentCLEF 2025 …","ref":"/talentclef/docs/talentclef-2025/workshop/","tags":"","title":"Workshop"},{"body":" We are thrilled to announce the release of the sample set for the TalentCLEF 2025! This sample dataset serves as an essential resource for participants to familiarize themselves with the format, scope, and types of data they will encounter in the TalentCLEF tasks.\nThe sample set provides a practical preview, allowing participants to explore the dataset structure and begin preliminary analyses in preparation for the workshop. The release includes detailed documentation including a Corpus Description and the Dataset Format\nYou can download the sample set in Zenodo clicking the button below:\nAccess the Zenodo download page We encourage participants to review these resources closely, as they provide critical context for tackling the workshop’s upcoming challenges. For any questions or further information, please reach out via the TalentCLEF website. We look forward to seeing the innovative ways participants will use this data to address TalentCLEF’s objectives!\n","categories":"","description":"","excerpt":" We are thrilled to announce the release of the …","ref":"/talentclef/blog/2024/10/25/release-of-sample-set-for-talentclef-2025/","tags":"","title":"Release of Sample Set for TalentCLEF 2025"},{"body":"Last week, on September 12, we presented the TalentCLEF Task at the CLEF2024 conference in Grenoble. This session provided an interesting opportunity to showcase the objectives, structure and expected impact of the task, generating a great deal of interest among researchers and practitioners alike.\nThe presentation highlighted the various facets of the TalentCLEF Task, including the unique challenges it addresses, the dataset design, and the evaluation criteria that participants will navigate.\nWe look forward to further promoting the task to interested participants through our TalentCLEF website.\nWe will be publishing the sample set soon.\nTalentCLEF presention @ CLEF2024\n","categories":"","description":"","excerpt":"Last week, on September 12, we presented the …","ref":"/talentclef/blog/2024/09/19/presentation-of-talentclef-task-at-clef2024-in-grenoble/","tags":"","title":"Presentation of TalentCLEF Task at CLEF2024 in Grenoble"},{"body":"We are excited to announce the launch of the new TalentCLEF workshop website!\nThe website is designed to be a comprehensive resource for participants and researchers interested in the upcoming TalentCLEF 2025 workshop, providing an easy access to essential information about the event, including detailed descriptions of the workshop’s goals, schedule, and registration process.\nYou can track all the details about the TalentCLEF task in the task item, including the schedule, summary of the tasks, and new information about the data and evaluation criteria will be published soon.\nTalentCLEF website screenshot\n","categories":"","description":"","excerpt":"We are excited to announce the launch of the new …","ref":"/talentclef/blog/2024/08/29/release-of-talentclef-website/","tags":"","title":"Release of TalentCLEF website"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: …","ref":"/talentclef/blog/","tags":"","title":"News"},{"body":" Links to TalentCLEF related resources:\nGithub Official Github page of TalentCLEF CLEF 2025 CLEF 2025 website TalentCLEF 2025 schedule TalentCLEF task calendar TalentCLEF 2025 Corpora TalentCLEF 2025 Corpora TalentCLEF Task A - Codabench TalentCLEF 2025 Task A CodaBench TalentCLEF Task B - Codabench TalentCLEF 2025 Task B CodaBench TalentCLEF 2025 Evaluation Script TalentCLEF 2025 Evaluation Script ","categories":"","description":"","excerpt":" Links to TalentCLEF related resources:\nGithub …","ref":"/talentclef/quick_links/","tags":"","title":"Quick links"},{"body":"","categories":"","description":"","excerpt":"","ref":"/talentclef/search/","tags":"","title":"Search Results"},{"body":" Si no eres redirigido automáticamente, haz clic en este enlace.\n","categories":"","description":"","excerpt":" Si no eres redirigido automáticamente, haz clic …","ref":"/talentclef/docs/talentclef-2025/","tags":"","title":""},{"body":" The deadline for submitting your Working Notes is 4th June! Please make sure to submit your Working Notes via EasyChair for CLEF2025. Don’t miss the deadline to ensure your work is included in the official CLEF 2025 proceedings! Here you can find the guidelines to upload your work! Current task schedule Status Date Event Link ✔ 9th September 2024 Website release ✔ 25th October 2024 Sample set release Link ✔ 13th November 2024 Registration opens Link ✔ 20th January 2025 Training data available for Tasks A and B Link ✔ 17th February 2025 Start of Task A with the release of the development data Link ✔ 17th March 2025 Start of Task B with the release of the development data Link ✔ 21st April 2025 Task A Test set release Link ✔ 21st April - 5th10th May 2025 Evaluation period of Task A Codabench Task A ✔ 28st April 2025 Task B Test set release Link ✔ 28st April - 5th10th May 2025 Evaluation period of Task B Codabench Task B ✔ 7th May 202512th May 2025 Publication of Official Results Task A and Task B 30th May 20254th June 2025 Submission of CLEF 2025 Working Notes 4th June 2025 - 27th June Review of Working Notes 27th June 2025 Notification of Acceptance for Participant Papers ","categories":"","description":"","excerpt":" The deadline for submitting your Working Notes is …","ref":"/talentclef/docs/","tags":"","title":"The task"}]