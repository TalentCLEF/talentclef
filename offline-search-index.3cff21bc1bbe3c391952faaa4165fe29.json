[{"body":" Luis Gasc√≥, PhD. Avature, Spain Hermenegildo Fabregat, PhD. Avature, Spain Laura Garc√≠a-Sardi√±a Avature, Spain Daniel Deniz Cerpa, PhD. Avature, Spain Paula Estrella Avature, Spain Alvaro Rodrigo, PhD. UNED, Spain Rabih Zbib, PhD. Avature, Spain ","categories":["Examples"],"description":"TalentCLEF 2025 is organised by:\n","excerpt":"TalentCLEF 2025 is organised by:\n","ref":"/talentclef/docs/talentclef-2025/people/task_organizers/","tags":["test","sample","docs"],"title":"Task Organizers"},{"body":" Task A Luis Gasc√≥, PhD. Avature, Spain Hermenegildo Fabregat, PhD. Avature, Spain Laura Garc√≠a-Sardi√±a Avature, Spain Daniel Deniz Cerpa, PhD. Avature, Spain Paula Estrella Avature Casimiro P√≠o Carrino Avature Alvaro Rodrigo, PhD. UNED, Spain Rabih Zbib, PhD. Avature Task B Jens-Joris Decorte TechWolf Matthias De Lange, PhD. TechWolf Warre Veys TechWolf Luis Gasc√≥, PhD. Avature ","categories":["Examples"],"description":"TalentCLEF 2026 is organised by:\n","excerpt":"TalentCLEF 2026 is organised by:\n","ref":"/talentclef/docs/talentclef-2026/people/task_organizers/","tags":["test","sample","docs"],"title":"Task Organizers"},{"body":" We take the legal and ethical implications of using AI in Human Resources very seriously. It is important to note that the data we utilize inherently excludes any personal information, focusing solely on job titles and skills without involving personal/company information or geographic location. This page contains the corpus information for TalentCLEF 2025 tasks. You can access the link to download in datasets page.\nTask A: Multilingual Job Title Matching Corpus Summary: The corpus used for Task A consists of a set of job titles in three languages: English, Spanish and German, from different job domains and professional sectors. These job titles have been collected and processed in order to facilitate the identification and comparison of equivalent titles across languages.\nThe training corpus has been generated using public terminologies, ensuring that the job titles are representative of a wide range of job domains and aligned with standard market terminology.\nOn the other hand, the validation and test corpora have been annotated by domain experts, following well-defined guidelines to ensure consistency and quality of labels. This annotation process, performed with specialized tools, included several stages of quality control to ensure that the labels were accurate and that the annotated titles accurately reflected the relationships between the different languages in a work environment.\nData:\nTraining Set: The training data is provided in a tabular format with three columns:\nfamily_id: The ISCO family id representing the group to which the job identifier belongs. id: An ESCO identifier indicating the origin of the pair‚Äôs job titles. jobtitle_1: The first job title in the pair. jobtitle_2: A second job title related to jobtitle_1. Each dataset is provided in separate files for each language involved in the task. The files are named according to the language, with the following format:\ntaskA_training_en.tsv: Contains related job titles in English. taskA_training_es.tsv: Contains related job titles in Spanish. taskA_training_de.tsv: Contains related job titles in German. An example of the content of these files is shown below:\nfamily_id id jobtitle_1 jobtitle_2 http://data.europa.eu/esco/isco/C2512 http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 desarrollador de software desarrolladora de soluciones http://data.europa.eu/esco/isco/C2512 http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 desarrollador de software ingeniera de aplicaciones http://data.europa.eu/esco/isco/C2512 http://data.europa.eu/esco/occupation/d0aa0792-4345-474b-9365-686cf4869d2e dise√±ador de software ingeniero de software Validation Set: The validation set is structured into three diferent files: queries, corpus elements and q_rels, and is provided separately for each language.\nQueries: The queries file contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: The corpus elements file contains the following fields:\nc_id: A unique identifier for each corpus element. jobtitle: The job title present in the corpus. qrels: This file defines the relationship between the query and the corpus elements. It does not include a column header, but one is shown here for illustrative purposes.\nq_id: The identifier of the query. iter: A reserved field (always 0). c_id: The identifier of the corresponding corpus element. relevance: A binary score (0 or 1) indicating the relevance of the corpus element to the query, where 1 signifies relevant and 0 non-relevant. We will provide validation set in english, spanish, german and chinese.\nExample of the content of these files for english:\nqueries q_id jobtitle 1 3d animator corpus_elements c_id jobtitle 1 animation artist 2 3d character animator 3 character technical director 4 character designer 5 animation lead 6 3d generalist 7 animator 8 character rigger 9 character animator q_rels q_id iter c_id relevance 1 0 2 1 1 0 3 1 1 0 4 1 1 0 5 1 1 0 6 1 1 0 7 1 1 0 8 1 1 0 9 1 Test Set: The test set is provided with two files: queries and corpus elements, and is provided separately for each language. For every language pair, participants need to generate a TREC Run File that adheres to the format specified in the submission format section. The data structure is similar to the one from the validation files.\nQueries: Contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: Contains:\nq_id: A unique identifier for each corpus element. jobtitle: The job title from the corpus element. Task B: Job Title-Based Skill Prediction Corpus Summary:\nThe dataset is designed to support job title-based skill prediction tasks in English across various job domains and professional sectors. It includes job titles and associated skills collected and processed to facilitate the training of models to solve this task.\nAs with Task A, the training data uses public terminologies to represent a broad spectrum of job domains, while the validation and test sets are annotated by domain experts. This expert annotation follows strict guidelines and quality control measures to ensure consistent labeling and accurate representation of job-title-to-skill relationships.\nData:\nTraining Set: For generating the training data for Task B, the information available in ESCO has been used. We have prepared the training data in three separate files: job2skill.tsv, jobid2terms.json and skillid2terms.json.\njob2skill.tsv: This file has been curated to include the most representative skills for each job title in ESCO. A filtering process has been applied to the number of skills per job title to avoid outliers. This file contains three columns:\njob_id: ESCO identifier for the job position. skill_id: ESCO identifier for the skill. rel_type:Indicator specifying whether the skill_id is essential or optional for a specific job_id. It can have the value ‚Äúessential‚Äù or ‚Äúoptional.‚Äù An example of the content of this file is shown below:\njob_id skill_id rel_type http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/8b94aa1e-89c9-459d-b3b4-1dfab8dec2df essential http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/f84a433f-34f1-4083-b0a3-24802623509c essential http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/fd33c66c-70c4-40e6-b87c-5495bd3bf26e optional jobid2terms.json: This JSON file contains job_id identifiers used in the training set for Task A as keys, and a list of valid lexical variants for each identifier as values.\n{ \"http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1\": [ \"application developer\", \"application programmer\", \"applications engineer\", \"application software developer\", \"battery software developer\", \"developer of software\", \"programmer\", \"soft developer\", \"software developer\", \"software developers\", \"software engineer\", \"software specialist\", \"solutions developer\" ] ... } skillid2terms.json: This JSON file contains skill_id identifiers as keys, and a list of valid lexical variants for each identifier as values.\n{ \"http://data.europa.eu/esco/skill/f84a433f-34f1-4083-b0a3-24802623509c\": [ \"web services\", \"web services systems\" ], \"http://data.europa.eu/esco/skill/fd33c66c-70c4-40e6-b87c-5495bd3bf26e\": [ \"design user interface\" ] } Validation Set: The validation set is divided into three diferent files: queries, corpus elements and q_rels:\nQueries: Contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: Contains:\nc_id: A unique identifier for each corpus element. esco_uri: The ESCO URIs associated to c_id. skill_aliases: The list aliases of the ESCO skill q_rels: This file maps the relationship between the query and the corpus elements:\nq_id: The identifier of the query. iter: A reserved field (always 0). c_id: The identifier of the corresponding corpus element. relevance: A binary score (0 or 1) indicating the relevance of the corpus element to the query, where 1 signifies relevant and 0 non-relevant. Example of the content of these files:\nqueries q_id jobtitle dev_qb_jt_1 corporate governance analyst corpus_elements c_id esco_uri skill_aliases dev_cb_sk_1 http://data.europa.eu/esco/skill/1c460d2d-90c6-4fc9-ad49-febb6e15605a [‚Äòpricing plans‚Äô, ‚Äòprice strategies‚Äô, ‚Äòpricing tactics‚Äô, ‚Äòpricing strategies‚Äô, ‚Äòpricing strategy‚Äô] dev_cb_sk_2 http://data.europa.eu/esco/skill/301a6581-e983-4bb6-8b31-b3ee2cbc2392 [‚Äòputting out fires‚Äô, ‚Ä¶, ‚Äòfires putting out‚Äô] dev_cb_sk_3 http://data.europa.eu/esco/skill/a4881e54-6055-4e61-855a-0a56ced7cfa3 [‚Äòonline assessment‚Äô, ‚Äòanalysis of web strategy‚Äô, ‚Äòweb presence assessment‚Äô, ‚Äòweb strategy assessment‚Äô] dev_cb_sk_4 http://data.europa.eu/esco/skill/efda73b4-5212-40a7-b2f8-d2f754ffdf2b [‚Äòkeeping up with trends‚Äô, ‚Äòkeep pace with trends‚Äô, ‚Äòfollow trends‚Äô, ‚Ä¶, ‚Äòkeep up with trends‚Äô] dev_cb_sk_5 http://data.europa.eu/esco/skill/22a173f5-868c-4d82-87e6-beed500ce070 [‚Äòprepare tax returns form‚Äô, ‚Ä¶, ‚Äòmake tax returns forms ready‚Äô, ‚Äòpreparing tax returns forms‚Äô] dev_cb_sk_6 http://data.europa.eu/esco/skill/97b890ff-acd7-46ad-8d3a-4186f4d42bbf [‚Äôtuning procedures‚Äô, ‚Ä¶, ‚Äôtuning skills‚Äô, ‚Äôtuning techniques‚Äô] dev_cb_sk_7 http://data.europa.eu/esco/skill/d5c20065-1d1f-446b-8143-9d1e180c512b [‚Äòiconography methods‚Äô, ‚Äòiconography‚Äô] q_rels q_id iter c_id relevance dev_qb_jt_1 0 dev_cb_sk_1034 1 dev_qb_jt_1 0 dev_cb_sk_1087 1 dev_qb_jt_1 0 dev_cb_sk_1088 1 dev_qb_jt_1 0 dev_cb_sk_1099 1 dev_qb_jt_1 0 dev_cb_sk_1104 1 dev_qb_jt_1 0 dev_cb_sk_1107 1 dev_qb_jt_1 0 dev_cb_sk_1110 1 dev_qb_jt_1 0 dev_cb_sk_1112 1 Test Set: The test set consists of two files: queries and corpus elements. Participants are required to produce a TREC Run File adhering to the structure outlined in the submission format section. The test set includes a background set in the queries.\nQueries: Contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: Contains:\nc_id: A unique identifier for each corpus element. esco_uri: The ESCO URIs associated to c_id. skill_aliases: The list aliases of the ESCO skill ","categories":["Examples"],"description":"","excerpt":" We take the legal and ethical implications of ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/data/description_corpus/","tags":["test","sample","docs"],"title":"Description of the Corpus"},{"body":" We take the legal and ethical implications of using AI in Human Resources very seriously. It is important to note that the data we utilize inherently excludes any personal information, focusing solely on job titles and skills without involving personal/company information or geographic location. Dataset files and detailed format specifications will be added here as they are released according to the official schedule. This page contains the corpus information for TalentCLEF 2026 tasks. You can access the link to download in datasets page.\nTask A: Contextualized Job-Person Matching Summary: The Task A corpus consists of job descriptions and r√©sum√©s (CVs) in two languages: English and Spanish. Documents are synthetically generated from structured data derived from real job descriptions and curricula vitae, preserving realism while protecting privacy. The dataset covers multiple professional domains and sectors, selected via clustering over semantic vector representations to ensure diversity and representativeness.\nSynthetic documents were generated in English and then manually reviewed to ensure quality and internal coherence. Job‚Äìcandidate matches were annotated by human experts, who determined whether each r√©sum√© is suitable for a given job offer. The Spanish version was created through parallel translation using LLMs and validated by human reviewers to ensure semantic consistency and content equivalence across languages.\nData:\nTraining Set:\nNo training set is provided for this task. However, participants are encouraged to use external resources or data from previous TalentCLEF editions if needed for their problem modeling and solution development.\nDevelopment Set:\nFor each language (English and Spanish),the development set is structured into three different components: queries, corpus, and qrels.tsv.\nQueries: A folder containing 10 job description files. Each file is named with its unique identifier (e.g., 1234, 5678), and the file name serves as the q_id for that query.\nCorpus: A folder containing 472 r√©sum√© files. Each file is named with its unique identifier (e.g., 1, 2, 3), and the file name serves as the c_id for that corpus document.\nqrels.tsv: This file defines the relationship between the queries and the corpus elements. It does not include a column header, but one is shown here for illustrative purposes:\nq_id: The identifier of the query (corresponding to the query file name). iter: A reserved field (always 0). c_id: The identifier of the corresponding corpus element (corresponding to the corpus file name). relevance: A binary score (0 or 1) indicating the relevance of the corpus element to the query, where 1 signifies relevant and 0 non-relevant. Example structure of qrels.tsv:\nq_id iter c_id relevance 1234 0 1 1 1234 0 5 0 1234 0 12 1 5678 0 2 1 5678 0 8 0 Test Set:\nTask B: Job-Skill Matching with Skill Type Classification Summary:\nThis dataset supports job title‚Äìbased skill prediction in English across multiple job domains and professional sectors. It includes job titles and associated skills, curated and processed to facilitate training and evaluation for this task.\nParticipants must retrieve the skills from a gazetteer that best match each job title and classify each retrieved skill as core or contextual. Core skills are required to perform a job regardless of employer or work setting (i.e., essential for the role). Contextual skills depend on factors such as the organization, industry, or specific project, and can therefore be considered optional.\nFor example, a software engineer may be required to develop mobile applications in some contexts depending on the product and technical stack, but will generally be expected to write code. In this case, write code is a core skill, while mobile application development is a contextual skill.\nData:\nTraining Set:\nFor generating the training data for Task B, the information available in ESCO has been used. We have prepared the training data in three separate files: job2skill.tsv, jobid2terms.json and skillid2terms.json.\njob2skill.tsv: This file has been curated to include the most representative skills for each job title in ESCO. A filtering process has been applied to the number of skills per job title to avoid outliers. This file contains three columns:\njob_id: ESCO identifier for the job position. skill_id: ESCO identifier for the skill. rel_type: Indicator specifying whether the skill_id is core or contextual for a specific job_id. It can have the value ‚Äúessential‚Äù or ‚Äúoptional‚Äù, labels that are aligned to the ‚Äúcore‚Äù and ‚Äúcontextual‚Äù labels that will be used in the development and test set. An example of the content of this file is shown below:\njob_id skill_id rel_type http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/8b94aa1e-89c9-459d-b3b4-1dfab8dec2df essential http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/f84a433f-34f1-4083-b0a3-24802623509c essential http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1 http://data.europa.eu/esco/skill/fd33c66c-70c4-40e6-b87c-5495bd3bf26e optional jobid2terms.json: This JSON file contains job_id identifiers used in the training set for Task B as keys, and a list of valid lexical variants for each identifier as values.\n{ \"http://data.europa.eu/esco/occupation/f2b15a0e-e65a-438a-affb-29b9d50b77d1\": [ \"application developer\", \"application programmer\", \"applications engineer\", \"application software developer\", \"battery software developer\", \"developer of software\", \"programmer\", \"soft developer\", \"software developer\", \"software developers\", \"software engineer\", \"software specialist\", \"solutions developer\" ] ... } skillid2terms.json: This JSON file contains skill_id identifiers as keys, and a list of valid lexical variants for each identifier as values.\n{ \"http://data.europa.eu/esco/skill/f84a433f-34f1-4083-b0a3-24802623509c\": [ \"web services\", \"web services systems\" ], \"http://data.europa.eu/esco/skill/fd33c66c-70c4-40e6-b87c-5495bd3bf26e\": [ \"design user interface\" ] } Validation Set:\nThe validation set is divided into three different files: queries, corpus elements and q_rels:\nQueries: Contains the following fields:\nq_id: A unique identifier for the query. jobtitle: The job title used as the query. Corpus Elements: Contains:\nc_id: A unique identifier for each corpus element. esco_uri: The ESCO URIs associated to c_id. skill_aliases: The list aliases of the ESCO skill. q_rels: This file maps the relationship between the query and the corpus elements:\nq_id: The identifier of the query. iter: A reserved field (always 0). c_id: The identifier of the corresponding corpus element. relevance: A binary score (0, 1 or 2) indicating the relevance of the corpus element to the query, where 2 signifies Core Skill, 1 signifies Contextual skill and 0 non-relevant. Example of the content of these files:\nqueries q_id jobtitle dev_qb_jt_64 senior sw engineer corpus_elements c_id esco_uri skill_aliases dev_cb_sk_320 http://data.europa.eu/esco/skill/54924a2c-daca-40d3-9716-4b38ceb04f38 [‚Äòalgorithms‚Äô, ‚Äòalgorithmic‚Äô, ‚Äòalgorithm‚Äô, ‚Äòformulae‚Äô, ‚Äòformulas‚Äô] dev_cb_sk_324 http://data.europa.eu/esco/skill/2857540c-180b-4208-9127-e94a01871966 [‚Äòalign software with system architectures‚Äô] dev_cb_sk_409 http://data.europa.eu/esco/skill/f28617ad-afdd-4041-814c-216153a38998 [‚Äòanalyse software specifications‚Äô, ‚Äôevaluate software design specification‚Äô, ‚Äôexamine software requirements‚Äô, ‚Ä¶] dev_cb_sk_1517 http://data.europa.eu/esco/skill/21d2f96d-35f7-4e3f-9745-c533d2dd6e97 [‚Äòcomputer programming‚Äô, ‚ÄòDIBOL‚Äô, ‚ÄòSeed7‚Äô, ‚ÄòVisual Basic .NET‚Äô, ‚ÄòHyperTalk‚Äô, ‚ÄòKorn-shell‚Äô, ‚Ä¶] dev_cb_sk_316 http://data.europa.eu/esco/skill/8c88d336-c249-4537-b26e-d679a85c4b9b [‚ÄòAjax Framework‚Äô] dev_cb_sk_333 http://data.europa.eu/esco/skill/47a49cd6-097d-457a-9f7b-c290c14930d5 [‚Äòanalyse big data‚Äô, ‚Äòanalysing big data‚Äô, ‚Äòsearch big data‚Äô, ‚Äòbig data analysing‚Äô, ‚Äòscrutinise big data‚Äô, ‚Äôtest big data‚Äô, ‚Ä¶] qrels q_id iter c_id relevance dev_qb_jt_64 0 dev_cb_sk_320 2 dev_qb_jt_64 0 dev_cb_sk_324 2 dev_qb_jt_64 0 dev_cb_sk_409 2 dev_qb_jt_64 0 dev_cb_sk_1517 2 dev_qb_jt_64 0 dev_cb_sk_316 1 dev_qb_jt_64 0 dev_cb_sk_333 1 Test Set:\n","categories":["Examples"],"description":"","excerpt":" We take the legal and ethical implications of ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/data/description_corpus/","tags":["test","sample","docs"],"title":"Description of the Corpus"},{"body":" In today‚Äôs rapidly changing socio-technological landscape, industries and workplaces are transforming quickly. Technological advancements, such as task automation and Artificial Intelligence (AI), are reshaping the labor market by creating new roles that demand specialized skills, often difficult to source. The rise of remote hiring, fueled by technological innovation, has expanded the labor market to a global and multilingual scale. Simultaneously, social progress is narrowing ethnic and gender disparities within companies, fostering more inclusive workplaces.\nSimultaneously, there has been rapid progress in the development and deployment of language-based systems, driven in part by the creation of the Large Language Models (LLMs). These advances are revolutioning the use of tecnology in Human Capital Management (HCM) and Human Resources (HR), enabling the generation of systems able to process large volumens of data and facilitate the identification of the best candidates for specific roles based on their resumee information.\nIntegrating language technologies into HCM significantly enhances key areas. In sourcing and hiring, these tools improve candidate matching by analyzing their skills and experience. During onboarding and training, they create personalized learning materials tailored to individual employee needs. For strategic workforce planning, NLP tools predict market skill trends and future company demands. Additionally, in career development, these technologies monitor employee progress, supporting targeted upskilling and reskilling aligned with both organizational goals and personal aspirations.\nDespite all these benefits, the development and implementation of these systems present challenges such as:\nMultilingualism: The global nature of modern workforces means that companies often need to manage employees and candidates who speak multiple languages. This requires language-based systems to not only understand and process various languages accurately but also to maintain the context and cultural nuances inherent in each. Developing systems that effectively handle multiple languages is a complex task that involves significant computational resources and sophisticated NLP techniques.\nFair models: Ensuring fairness and reducing bias is a critical challenge in HCM. These systems can inadvertently perpetuate existing biases present in the data they are trained on, which can affect hiring decisions, employee evaluations, and promotions. Creating fair and unbiased models requires careful data curation, continuous monitoring, and algorithmic adjustments to mitigate biases related to gender, ethnicity, and other social factors.\nCross-Industry Adaptability: NLP systems must be flexible enough to align with the unique requirements, standards, and practices of each sector, from healthcare to technology to retail, ensuring they are effective and relevant in various contexts.\nThe first edition of TalentCLEF aims to develop and evaluate models designed to facilitate three essential tasks:\nFinding/ranking candidates for job positions based on their experience and professional skills. Implementing upskilling and reskilling strategies that promote the coninuous development of workers Detecting emerging skills and skills gaps of importance in organizations. ","categories":"","description":"","excerpt":" In today‚Äôs rapidly changing socio-technological ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/motivation/","tags":"","title":"Motivation"},{"body":" In today‚Äôs rapidly changing socio-technological landscape, industries and workplaces are transforming quickly. Technological advancements, such as task automation and Artificial Intelligence (AI), are reshaping the labor market by creating new roles that demand specialized skills, often difficult to source. The rise of remote hiring, fueled by technological innovation, has expanded the labor market to a global and multilingual scale. Simultaneously, social progress is narrowing ethnic and gender disparities within companies, fostering more inclusive workplaces.\nSimultaneously, there has been rapid progress in the development and deployment of language-based systems, driven in part by the creation of the Large Language Models (LLMs). These advances are revolutioning the use of tecnology in Human Capital Management (HCM) and Human Resources (HR), enabling the generation of systems able to process large volumens of data and facilitate the identification of the best candidates for specific roles based on their resumee information.\nIntegrating language technologies into HCM significantly enhances key areas. In sourcing and hiring, these tools improve candidate matching by analyzing their skills and experience. During onboarding and training, they create personalized learning materials tailored to individual employee needs. For strategic workforce planning, NLP tools predict market skill trends and future company demands. Additionally, in career development, these technologies monitor employee progress, supporting targeted upskilling and reskilling aligned with both organizational goals and personal aspirations.\nDespite all these benefits, the development and implementation of these systems present challenges such as:\nMultilingualism: The global nature of modern workforces means that companies often need to manage employees and candidates who speak multiple languages. This requires language-based systems to not only understand and process various languages accurately but also to maintain the context and cultural nuances inherent in each. Developing systems that effectively handle multiple languages is a complex task that involves significant computational resources and sophisticated NLP techniques.\nFair models: Ensuring fairness and reducing bias is a critical challenge in HCM. These systems can inadvertently perpetuate existing biases present in the data they are trained on, which can affect hiring decisions, employee evaluations, and promotions. Creating fair and unbiased models requires careful data curation, continuous monitoring, and algorithmic adjustments to mitigate biases related to gender, ethnicity, and other social factors.\nCross-Industry Adaptability: NLP systems must be flexible enough to align with the unique requirements, standards, and practices of each sector, from healthcare to technology to retail, ensuring they are effective and relevant in various contexts.\nThe first edition of TalentCLEF aims to develop and evaluate models designed to facilitate three essential tasks:\nFinding/ranking candidates for job positions based on their experience and professional skills. Implementing upskilling and reskilling strategies that promote the coninuous development of workers Detecting emerging skills and skills gaps of importance in organizations. ","categories":"","description":"","excerpt":" In today‚Äôs rapidly changing socio-technological ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/motivation/","tags":"","title":"Motivation"},{"body":" The TalentCLEF 2025 task is structured into two independent sub-tasks, each taking account a particular use case scenario:\nTask A: Multilingual Job Title Matching Goal: Develop systems that can identify and rank job titles most similar to a given job title. For each job title in a provided test set, participants must generate a ranked list of similar job titles from a specified knowledge base.\nMultilingual: Participants are required to develop systems adapted to English, Spanish, German, and optionally, Chinese.\nData: More information about data will be shown in Data section, but essentially we will provide:\nTraining Set: A collection of 15,000 pairs of related job titles per language (English, Spanish, and German), each labeled with a concept identifier, will be provided to facilitate the creation of cross-language training samples. No training data will be provided in Chinese, opening the possibility to use different techniques to improve the performance of the models in this language.\nDevelopment Set: Participants will receive a manually annotated evaluation set of 100 samples per language, consisting of a job title and its list of related job titles. A knowledge base of 2,500 job titles in each task language will also be provided for participants to generate predictions by ranking it.\nTest set: A background set comprising 5,000 job titles will be provided. The evaluation, however, will be conducted on a subset of the background set, that will be a gold standard corpus of 100 job titles in each language annotated with the same methodology as the development set.\nEvaluation: Details about evaluation will be placed in Evaluation page, but the model performance will be evaluated with information retrieval metrics, being the Mean Average Precision (MAP) the official metric of the task, although results will be provided in other metrics such as Mean Reciprocal Rank (MRR) and Precision@K(1,5,10).\nTask B: Job Title-Based Skill Prediction Goal: Develop systems capable of retrieving relevant skills associated with a given job title.\nData: More information about data will be shown in Data section, but essentially we will provide:\nTraining set: A training set of at least 5.000 job titles along with the professional skills required for each position will be provided. This data is sourced from actual job descriptions and semi-automatically curated to ensure high accuracy in the training set.\nDevelopment set: The development set will consist of 200 job titles along with their related skills, normalized to ESCO terminology.\nTest set: The test set comprises a list of 500 job titles. The participants will be required to predict the related skills using the provided gazetteer.\nEvaluation: Details about evaluation will be placed in Evaluation page, but the model performance in this task will be also evaluated with information retrieval metrics, being the Mean Average Precision (MAP) the official metric of the task, although results will be provided in other metrics such as Mean Reciprocal Rank (MRR) and Precision@K(1,5,10).\n","categories":"","description":"","excerpt":" The TalentCLEF 2025 task is structured into two ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/task-summary/","tags":"","title":"Task Summary"},{"body":" The TalentCLEF 2026 task is structured into two independent sub-tasks, each taking account a particular use case scenario:\nTask A: Contextualized Job-Person Matching Goal: Develop systems capable of identifying and ranking the most suitable candidate r√©sum√©s for a given job offer. For each job description in the test set, participants must provide a ranked list of candidate profiles according to their relevance to the position.\nMultilingual: Participants are required to develop systems adapted to English and optionally systems that operate in cross-lingual scenarios involving English and Spanish.\nData: More information about data will be shown in Data section, but we will provide a synthetically generated dataset built from structured resources derived from real job descriptions and r√©sum√©s, containing no personal information, and without posing privacy risks\nDevelopment Set: Participants will receive a small manually annotated development set of 30 samples per language, consisting of a job description and its list of relevant r√©sum√©s. A knowledge base of around 100 r√©sum√©s will also be provided for participants to generate predictions by ranking it.\nTest set: A background set of job offers will be provided. The evaluation, however, will be conducted on a subset of the background set, that will be a gold standard corpus of 50 job offers in each language annotated with the same methodology as the development set.\nEvaluation: Details about evaluation will be placed in Evaluation page, but the model performance will be evaluated with information retrieval metrics, being the Mean Average Precision (MAP) the official metric of the task, although results will be provided in other metrics such as Mean Reciprocal Rank (MRR) and Precision@K(1,5,10).\nTask B: Job-Skill Matching with Skill Type Classification Goal: Develop systems capable of retrieving relevant skills associated with a given job title and additionally classifying each retrieved skill as either core, complementary or transversal.\nData: More information about data will be shown in Data section, but essentially we will provide:\nTraining set: A training set of at least 5.000 job titles along with the professional skills required for each position will be provided.\nDevelopment set: The development set will consist of 200 job titles along with their related skills, normalized to ESCO terminology. The data will include a new annotation layer indicating the relevance of each skill for the job position, specifying whether the skill is considered core or contextual. Core skills are those required to perform a specific job independently of the work context or employer and can thus be seen as essential for the position, whereas contextual skills depend on factors such as the organization or industry and can therefore be seen as optional.\nTest set: The test set comprises a list of 500 job titles. The participants will be required to predict the related skills using the provided gazetteer, but also the skill type classification.\nEvaluation: Details about evaluation will be shared soon in the Evaluation page, but the model performance will be evaluated with information retrieval metrics, such as the Normalized Discounted Cumulative Gain (nDCG) and Mean Average Precision (MAP), although results will be provided also using other metrics such as Mean Reciprocal Rank (MRR) and Precision@K(1,5,10).\n","categories":"","description":"","excerpt":" The TalentCLEF 2026 task is structured into two ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/task-summary/","tags":"","title":"Task Summary"},{"body":" We take the legal and ethical implications of using AI in Human Resources very seriously. It is important to note that the data we utilize inherently excludes any personal information, focusing solely on job titles and skills without involving personal/company information or geographic location. The data will be hosted on the Zenodo platform under the NLP in HR community, following the file structure outlined below. Each time new data is added, an updated version of the dataset will be published on the platform. Access the Zenodo download page The dataset structure on Zenodo is organized into two *.zip files, TaskA and TaskB, each containing training, validation and test folders to suuport different stages of model development. Until the official release of the full training set, users can access a sample version of the data through the sampleset_TaskA.zip and sampleset_TaskB.zip files.\nTaskA includes language-specific subfolders within the training and validation directories, covering English, Spanish, German, and Chinese job title data. The training folders for TaskA contain language-specific .tsv files for each respective language. Validation folders include three essential files‚Äîqueries, corpus_elements, and q_rels‚Äîfor evaluating model relevance to search queries. TaskA‚Äôs test folder has queries and corpus_elements files for testing every language considered. Participant can combine queries and corpus elements for the cross-lingual evaluation of the Task. The data can be found in the TaskA.zip file.\nüóúÔ∏èÔ∏è TaskA üìÅ training üìÅ english üìÑ taskA_training_en.tsv üìÅ spanish üìÑ taskA_training_es.tsv üìÅ german üìÑ taskA_training_de.tsv üìÅ validation üìÅ english üìÑ queries üìÑ corpus_elements üìÑ qrels.tsv üìÅ spanish üìÅ german üìÅ chinese üìÅ test üìÅ english üìÑ queries üìÑ corpus_elements üìÅ spanish üìÅ german üìÅ chinese TaskB follows a similar structure but without language-specific subfolders, providing general .tsv files for training, validation, and testing. This consistent file organization enables efficient data access and structured updates as new data versions are published. The data can be found in the TaskB.zip file.\nüóúÔ∏èÔ∏è TaskB üìÅ training üìÑ job2skill.tsv üìÑ jobid2terms.json üìÑ skillid2terms.json üìÅ validation üìÑ queries üìÑ corpus_elements üìÑ q_rels üìÅ test üìÑ queries üìÑ corpus_elements ","categories":["Examples"],"description":"","excerpt":" We take the legal and ethical implications of ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/data/datasets/","tags":["test","sample","docs"],"title":"Datasets"},{"body":" We take the legal and ethical implications of using AI in Human Resources very seriously. It is important to note that the data we utilize inherently excludes any personal information, focusing solely on job titles and skills without involving personal/company information or geographic location. The data will be hosted on the Zenodo platform under the NLP in HR community, following the file structure outlined below. Each time new data is added, an updated version of the dataset will be published on the platform. Access the Zenodo download page The dataset structure on Zenodo is organized into two *.zip files, TaskA.zip and TaskB.zip, each containing folders to support different stages of model development. So far, only the development set of Task A and the training set of Task A have been released, but in future releases, as the tasks progress, additional data will be added to the different subfolders for each task.\nTaskA includes language-specific subfolders within the directories, covering English and Spanish data. Development folders include two essential folders (queries, corpus), and a qrels file for evaluating model relevance to search queries.\nüóúÔ∏èÔ∏è TaskA üìÅ development üìÅ english üìÅ queries üìÑ 1234 ... üìÅ corpus üìÑ 1 ... üìÑ qrels.tsv üìÅ spanish üìÅ queries üìÑ 9865 ... üìÅ corpus üìÑ 2 ... üìÑ qrels.tsv üìÅ test TaskB follows a similar structure but without language-specific subfolders, providing general .tsv files for training, validation, and testing. This consistent file organization enables efficient data access and structured updates as new data versions are published.\nüóúÔ∏èÔ∏è TaskB üìÅ training üìÑ job2skill.tsv üìÑ jobid2terms.json üìÑ skillid2terms.json üìÅ validation üìÑ queries üìÑ corpus_elements üìÑ q_rels üìÅ test ","categories":["Examples"],"description":"","excerpt":" We take the legal and ethical implications of ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/data/datasets/","tags":["test","sample","docs"],"title":"Datasets"},{"body":" Eneko Agirre - Full Professor of the University of the Basque Country UPV/EHU - ACL Fellow David Camacho - Full Professor of the Technical University of Madrid (UPM) Debora Nozza - Assistant Professor of Bocconi University Jens-Joris Decorte - Lead AI Scientist at TechWolf David Graus - Lead Data Scientist at Randstad Group Mesutt Kayaa - Postdoctoral Researcher at Jobindex A/S and IT University Copenhagen Jan Luts - Senior Data Scientist at NTT Data \u0026 ESCO Elena Montiel-Ponsoda - Professor at the Technical University of Madrid (UPM) - AI4Labour project Javier Huertas Tato - Assistant Professor of the Technical University of Madrid (UPM) Patricia Mart√≠n Chozas - Postdoctoral Researcher at the Ontology Engineering Group (UPM) - AI4Labour project ","categories":["Examples"],"description":"Scientific committee of the TalentCLEF 2025 Task\n","excerpt":"Scientific committee of the TalentCLEF 2025 Task\n","ref":"/talentclef/docs/talentclef-2025/people/scientific_committee/","tags":["test","sample","docs"],"title":"Scientific Committee"},{"body":"The Scientific Committee will be announced soon.\n","categories":["Examples"],"description":"Scientific committee of the TalentCLEF 2026 Task\n","excerpt":"Scientific committee of the TalentCLEF 2026 Task\n","ref":"/talentclef/docs/talentclef-2026/people/scientific_committee/","tags":["test","sample","docs"],"title":"Scientific Committee"},{"body":" The deadline for submitting your Working Notes is 4th June! Please make sure to submit your Working Notes via EasyChair for CLEF2025. Don‚Äôt miss the deadline to ensure your work is included in the official CLEF 2025 proceedings! Here you can find the guidelines to upload your work! Registration for TalentCLEF 2025 is now closed.\n","categories":"","description":"Information about registration procedures","excerpt":"Information about registration procedures","ref":"/talentclef/docs/talentclef-2025/registration/","tags":"","title":"Registration"},{"body":" Info. Registration is open. You can register here. Registration for TalentCLEF 2026 is now open. You can register here. Once registered, participants will be periodically informed about new dataset releases, as well as the different phases and deadlines of the evaluation campaign.\n","categories":"","description":"Information about registration procedures","excerpt":"Information about registration procedures","ref":"/talentclef/docs/talentclef-2026/registration/","tags":"","title":"Registration"},{"body":" The Codabench evaluation platform will be available on 2nd March 2026. Once live, this page will be updated with links to the evaluation framework, detailed submission guidelines, and the evaluation procedure. The evaluation for TalentCLEF-2026 will be conducted on Codabench. Submissions will be ranked using Mean Average Precision (MAP) and nDCG (normalized Discounted Cumulative Gain).\nFor local testing, an evaluation script is available in our GitHub repository. You can access it here: Evaluation Script. You can find a tutorial on the process of generating submission files and evaluating them in the Tutorials section of the Additional resources page\nTalentCLEF Task A - Codabench TalentCLEF Task B - Codabench Evaluation dates Task A: Codabench: Link Start Date: 13st April 2026 End Date: 3rd May 2026 Task B: Start Date: 13st April 2026 End Date: 3rd May 2026 Evaluation Criteria The top-performing teams will be determined based on the following evaluation criteria:\nTask A:\nBest Overall Multilingual Performance ‚Äì The highest-performing system across English and Spanish, measured as the average Mean Average Precision (MAP) in en-en and es-es. Best Cross-Lingual Performance ‚Äì The best-performing system in cross-lingual scenarios, calculated as the MAP in en-es. Best Bias-Controlled Model ‚Äì The system that minimizes performance differences across different gender groups. During the CLEF workshop, certificates will be awarded to the first and second-best systems in each category.\nTask B: The highest-performing system based on NDCG.\n","categories":["Examples"],"description":"","excerpt":" The Codabench evaluation platform will be ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/evaluation/evaluation_info/","tags":"","title":"Evaluation info"},{"body":" You can find a tutorial on the process of generating submission files and evaluating them in the Tutorials section of the Additional resources page Task A Submissions must adhere to the TREC Run File Format, including column names, with exactly six space-separated columns per line, structured as follows:\n\u003cq_id\u003e Q0 \u003cdoc_id\u003e \u003crank\u003e \u003cscore\u003e \u003crun_tag\u003e Column Descriptions\n\u003cq_id\u003e: The identifier for the query corresponding to the job description being searched. Q0: A constant placeholder (always set to ‚ÄúQ0‚Äù). \u003cdoc_id\u003e: The identifier for the retrieved candidate resum√©. \u003crank\u003e: The ranking position of the retrieved document for the query (starting at 1). \u003cscore\u003e: The retrieval score assigned to the document (higher scores indicate greater relevance). \u003crun_tag\u003e: A label identifying the submitted run (e.g., teamA_systemA). Example The submissions must NOT contains the column headers:\nquery1 Q0 ce1 1 0.6910377144813538 teamA_systemA query1 Q0 ce4 2 0.6690284013748169 teamA_systemA query1 Q0 ce5 3 0.662328839302063 teamA_systemA Task B AVailable soon!\n","categories":["Examples"],"description":"","excerpt":" You can find a tutorial on the process of ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/evaluation/submission_format/","tags":"","title":"Evaluation info"},{"body":" Thursday 11 September 14:15-15:45 Session 1 Welcome Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management\nLuis Gasco, Hermenegildo Fabregat, Laura Garc√≠a-Sardi√±a, Paula Estrella, Daniel Deniz, √Ålvaro Rodrigo, Rabih Zbib AlexU-NLP at TalentCLEF 2025: Curriculum-Driven Hybrid Retrieval for Multilingual Job Title Matching\nRana Barakat, Omar Mokhtar, Marwan Torki, Nagwa Elmakky UDII-UPM at TalentCLEF 2025: Task A - Multilingual Job Title Matching\nJavier Rodr√≠guez-Vidal, Ascensi√≥n L√≥pez-Vargas, Pablo Manuel Vigara Gallego, Francisco Javier Del √Ålamo, √Ångel Garc√≠a-Beltr√°n VerbaNex at TalentCLEF2025: Semantic Matching of Multilingual Job Titles through a Framework Integrating ESCO Taxonomy Melissa Moreno Novoa, Juan Carlos Martinez-Santos, Jairo Serrano, Edwin Puertas Multilingual Job Title Matching with MPNet-Based Sentence Transformers Adam Brikman, Michael Sana, Holden Ruegger A Two-Stage Multilingual Job Title Matching System: Combining Expert Knowledge and LLM-based Ranking\nMar Rodr√≠guez, Olatz Perez-de-Vi√±aspre, Naiara Perez 15:45-16:30 Poster session 16:30-18:00 Session 2 Keynote presentation: Human-AI Interaction for the Future of Work\nMarios Constantinides (CYENS Centre of Excellence, Cyprus) HULAT-UC3M at TalentCLEF: Artificial Intelligence and Natural Language Processing applied to HR Management\n√Ålvaro Tejera Villar, Isabel Segura Bedmar TechWolf at TalentCLEF 2025: Multilingual JobBERT-V2 for Cross-Lingual Job Title Matching\nJens-Joris Decorte, Matthias De Lange, Jeroen Van Hautte NLPnorth @ TalentCLEF 2025: Discriminative vs. Contrastive vs. Prompting for Job Title and Job-Skill Matching\nMike Zhang, Rob van der Goot Friday 12 September 11:00-12:30 Session 3 pjmathematician at TalentCLEF 2025: Enhancing Job Title and Skill Matching with GISTEmbed and LLM-Augmented Data\nPoojan Vachharajani Enhancing Human Capital Management: AI Techniques for Candidate Matching and Skill Extraction\nMuhammad Hasan Nizami, Ahtisham Uddin, Muhammad Talha Salani, Ayesha Saeed, Faisal Alvi, Abdul Samad COTECMAR‚ÄìUTB at TalentCLEF 2025: Linking Job Titles and ESCO Skills with Sentence Transformer Embeddings\nJhonattan Llamas, Edwin Puertas, Jairo Serrano, Juan Martinez Beyond Titles: Semantic Matching of Jobs and Skills Using LLMs and S-BERT\nIago X. V√°zquez, Rodrigo Sedano, Silvia Gonz√°lez, Javier Sedano Enhancing Job-Skill Matching with LLM-Driven Data Augmentation and Fine-Tuned Bi-Encoders\nMohab Ali Discussion, Awards Ceremony \u0026 Closing ","categories":["Examples"],"description":"","excerpt":" Thursday 11 September 14:15-15:45 Session 1 ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/workshop/workshop_schedule/","tags":"","title":"TalentCLEF 2025 Workshop Schedule"},{"body":"To support your participation in this shared task, we have compiled a list of additional resources that may be useful for understanding the task better, exploring related work, and utilizing domain-specific models.\n1. Related papers: Gasco, L., Fabregat, H., Garc√≠a-Sardi√±a, L., Deniz, D., Rodrigo, A., Estrella, P., \u0026 Zbib, R. (2025, April). TalentCLEF at CLEF2025: Skill and Job Title Intelligence for Human Capital Management. In European Conference on Information Retrieval (pp. 479-486). Link Zbib, R., Lacasa, L. A., Retyk, F., Poves, R., Aizpuru, J., Fabregat, H., ‚Ä¶ \u0026 Garc√≠a-Casademont, E. (2022). Learning Job Titles Similarity from Noisy Skill Labels. arXiv preprint arXiv:2207.00494 Deniz, D., Retyk, F., Garc√≠a-Sardi√±a, L., Fabregat, H., Gasco, L., \u0026 Zbib, R. (2024). Combined Unsupervised and Contrastive Learning for Multilingual Job Recommendation. Link CEUR Decorte, J. J., Van Hautte, J., Demeester, T., \u0026 Develder, C. (2021). Jobbert: Understanding job titles through skills. arXiv preprint arXiv:2109.09605 Anand, S., Decorte, J. J., \u0026 Lowie, N. (2022). Is it required? ranking the skills required for a job-title. arXiv preprint arXiv:2212.08553 Zhang, M., Van Der Goot, R., \u0026 Plank, B. (2023). ESCOXLM-R: Multilingual taxonomy-driven pre-training for the job market domain. arXiv preprint arXiv:2305.12092 Bhola, A., Halder, K., Prasad, A., \u0026 Kan, M. Y. (2020, December). Retrieving skills from job descriptions: A language model based extreme multi-label classification framework. In Proceedings of the 28th international conference on computational linguistics (pp. 5832-5842). Link Retyk, F., Gasco, L., Carrino, C. P., Deniz, D., \u0026 Zbib, R. (2024). MELO: An Evaluation Benchmark for Multilingual Entity Linking of Occupations. arXiv preprint arXiv:2410.08319. Laosaengpha, N., Tativannarat, T., Rutherford, A., \u0026 Chuangsuwanich, E. (2025). Mitigating Language Bias in Cross-Lingual Job Retrieval: A Recruitment Platform Perspective. arXiv preprint arXiv:2502.03220 Laosaengpha, N., Tativannarat, T., Piansaddhayanon, C., Rutherford, A., \u0026 Chuangsuwanich, E. (2024). Learning Job Title Representation from Job Description Aggregation Network. arXiv preprint arXiv:2406.08055 2. External Resources: ESCOXLM-R Model in Huggingface NESTA Taxonomy ESCO Taxonomy 3. Tutorials: We will publish a series of notebooks covering the fundamentals, including how to work with the data and upload predictions to Codalab.\nNotebook Colab Data Download and Load Task A - Prepare submission file and run evaluation Task A - Development set Baseline generation Task B - Prepare submission file and run evaluation Task A - Test set Baseline Generation Task B - Test set Baseline Generation ","categories":["Examples"],"description":"","excerpt":"To support your participation in this shared task, ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/data/additional_resources/","tags":["test","sample","docs"],"title":"Additional resources"},{"body":"To support your participation in this shared task, we have compiled a list of additional resources that may be useful for understanding the task better, exploring related work, and utilizing domain-specific models.\n1. Related papers: Gasco, L., Fabregat, H., Garc√≠a-Sardi√±a, L., Deniz, D., Rodrigo, A., Estrella, P., \u0026 Zbib, R. (2025, April). TalentCLEF at CLEF2025: Skill and Job Title Intelligence for Human Capital Management. In European Conference on Information Retrieval (pp. 479-486). Link Zbib, R., Lacasa, L. A., Retyk, F., Poves, R., Aizpuru, J., Fabregat, H., ‚Ä¶ \u0026 Garc√≠a-Casademont, E. (2022). Learning Job Titles Similarity from Noisy Skill Labels. arXiv preprint arXiv:2207.00494 Deniz, D., Retyk, F., Garc√≠a-Sardi√±a, L., Fabregat, H., Gasco, L., \u0026 Zbib, R. (2024). Combined Unsupervised and Contrastive Learning for Multilingual Job Recommendation. Link CEUR Decorte, J. J., Van Hautte, J., Demeester, T., \u0026 Develder, C. (2021). Jobbert: Understanding job titles through skills. arXiv preprint arXiv:2109.09605 Anand, S., Decorte, J. J., \u0026 Lowie, N. (2022). Is it required? ranking the skills required for a job-title. arXiv preprint arXiv:2212.08553 Zhang, M., Van Der Goot, R., \u0026 Plank, B. (2023). ESCOXLM-R: Multilingual taxonomy-driven pre-training for the job market domain. arXiv preprint arXiv:2305.12092 Bhola, A., Halder, K., Prasad, A., \u0026 Kan, M. Y. (2020, December). Retrieving skills from job descriptions: A language model based extreme multi-label classification framework. In Proceedings of the 28th international conference on computational linguistics (pp. 5832-5842). Link Retyk, F., Gasco, L., Carrino, C. P., Deniz, D., \u0026 Zbib, R. (2024). MELO: An Evaluation Benchmark for Multilingual Entity Linking of Occupations. arXiv preprint arXiv:2410.08319. Laosaengpha, N., Tativannarat, T., Rutherford, A., \u0026 Chuangsuwanich, E. (2025). Mitigating Language Bias in Cross-Lingual Job Retrieval: A Recruitment Platform Perspective. arXiv preprint arXiv:2502.03220 Laosaengpha, N., Tativannarat, T., Piansaddhayanon, C., Rutherford, A., \u0026 Chuangsuwanich, E. (2024). Learning Job Title Representation from Job Description Aggregation Network. arXiv preprint arXiv:2406.08055 2. External Resources: ESCOXLM-R Model in Huggingface NESTA Taxonomy ESCO Taxonomy 3. Tutorials: We will publish a series of notebooks covering the fundamentals, including how to work with the data and upload predictions to Codalab.\nNotebook Colab Data Download and Load Task A - Prepare submission file and run evaluation Task A - Development set Baseline generation Task B - Prepare submission file and run evaluation ","categories":["Examples"],"description":"","excerpt":"To support your participation in this shared task, ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/data/additional_resources/","tags":["test","sample","docs"],"title":"Additional resources"},{"body":" The evaluation for TalentCLEF-2025 will be conducted on Codabench. Submissions will be ranked using Mean Average Precision (MAP).\nTalentCLEF Task A - Codabench TalentCLEF Task B - Codabench Evaluation dates Task A: Start Date: 21st April 2025 End Date: 5th May 2025 Task B: Start Date: 28st April 2025 End Date: 5th May 2025 Evaluation Criteria The top-performing teams will be determined based on the following evaluation criteria:\nTask A:\nBest Overall Multilingual Performance ‚Äì The highest-performing system across English, Spanish, and German, measured as the average Mean Average Precision (MAP) in en-en, es-es, and de-de. Best Cross-Lingual Performance ‚Äì The best-performing system in cross-lingual scenarios, calculated as the average MAP in en-es and en-de. Best Bias-Controlled Model ‚Äì The system that minimizes performance differences across job titles in different gender groups. Additionally, there will be a special mention for the best-performing model in Chinese. During the CLEF workshop, certificates will be awarded to the first and second-best systems in each category.\nTask B: The highest-performing system based on MAP.\n","categories":["Examples"],"description":"","excerpt":" The evaluation for TalentCLEF-2025 will be ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/evaluation/evaluation_info/","tags":"","title":"Evaluation info"},{"body":" Links to the CEUR Proceedings here Overview paper Please, cite: Luis Gasco, Hermenegildo Fabregat, Laura Garc√≠a-Sardi√±a, Paula Estrella, Daniel Deniz, √Ålvaro Rodrigo and Rabih Zbib. 2025. Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management. In International Conference of the Cross-Language Evaluation Forum for European Languages. Springer. [View preprint on arXiv] [View Springer paper] BibTeX citation: @inproceedings{gasco2025overview, title={{Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management}}, author={Gasco, Luis and Fabregat, Hermenegildo and Garc\\'{\\i}a-Sardi{\\~n}a, Laura and Estrella, Paula and Deniz, Daniel and Rodrigo, \\'{A}lvaro and Zbib, Rabih}, booktitle={{Experimental IR Meets Multilinguality, Multimodality, and Interaction}}, publisher={{Springer Nature Switzerland}}, address={{Cham}}, pages={{464--485}}, isbn={{978-3-032-04354-2}}, year={2025}, } üìã Copy Copied! Participant papers Team Title Link Bibtex Iagox Semantic Matching of Jobs and Skills Using LLMs and S-BERT Link Copy bibtex DS@GT TalentCLEF Multilingual Job Title Matching with MPNet-Based Sentence Transformers Link Copy bibtex NLPnorth NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching Link Copy bibtex UDII-UPM UDII-UPM at TalentCLEF 2025: Task A-Multilingual Job Title Matching Link Copy bibtex moali Enhancing Job-Skill Matching with LLM-Driven Data Augmentation and Fine-Tuned Bi-Encoders Link Copy bibtex NT NT Team at Multilingual Job Title Matching Task A: Job Matching via Large Language Model-Based Description Generation and Retrieval Link Copy bibtex SCaLAR Fine-Tuned Sentence Transformer for Multilingual Job Title Matching Link Copy bibtex AlexU-NLP AlexU-NLP at TalentCLEF 2025: Curriculum-Driven Hybrid Retrieval for Multilingual Job Title Matching Link Copy bibtex SkillSeekers Enhancing Human Capital Management: AI Techniques for Candidate Matching and Skill Extraction Link Copy bibtex HULAT-UC3M HULAT-UC3M at TalentCLEF: ArtificialIntelligence and Natural Language Processing applied to HR Management Link Copy bibtex COTECMAR‚ÄìUTB Linking Job Titles and ESCO Skills with Sentence Transformer Embeddings Link Copy bibtex Ixa A Two-Stage Multilingual Job Title Matching System: Combining Expert Knowledge and LLM-based Ranking Link Copy bibtex pjmathematician pjmathematician at TalentCLEF 2025: Enhancing Job Title and Skill Matching with GISTEmbed and LLM-Augmented Data Link Copy bibtex TechWolf Multilingual JobBERT for Cross-Lingual Job Title Matching Link Copy bibtex VerbaNexAI VerbaNex at TalentCLEF2025: Semantic Matching of Multilingual Job Titles through a Framework Integrating ESCO Taxonomy Link Copy bibtex ","categories":["Examples"],"description":"","excerpt":" Links to the CEUR Proceedings here Overview paper ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/results/publications/","tags":"","title":"Publications"},{"body":" You can find a tutorial on the process of generating submission files and evaluating them in the Tutorials section of the Additional resources page Submissions must adhere to the TREC Run File Format, including column names, with exactly six space-separated columns per line, structured as follows:\n\u003cq_id\u003e Q0 \u003cdoc_id\u003e \u003crank\u003e \u003cscore\u003e \u003crun_tag\u003e Column Descriptions\n\u003cq_id\u003e: The identifier for the query corresponding to the job title being searched. Q0: A constant placeholder (always set to ‚ÄúQ0‚Äù). \u003cdoc_id\u003e: The identifier for the retrieved corpus element. \u003crank\u003e: The ranking position of the retrieved document for the query (starting at 1). \u003cscore\u003e: The retrieval score assigned to the document (higher scores indicate greater relevance). \u003crun_tag\u003e: A label identifying the submitted run (e.g., teamA_systemA). Example The submissions must contains the column headers:\nq_id Q0 doc_id rank score tag query1 Q0 ce1 1 0.6910377144813538 teamA_systemA query1 Q0 ce4 2 0.6690284013748169 teamA_systemA query1 Q0 ce5 3 0.662328839302063 teamA_systemA ","categories":["Examples"],"description":"","excerpt":" You can find a tutorial on the process of ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/evaluation/submission_format/","tags":["test","sample","docs"],"title":"Submission format"},{"body":" Below you will find the official results for Task A of TalentCLEF. This section provides both a graphical overview and a detailed table of the full results. The interactive visualizations are designed to help you explore and analyze the performance of participating systems based on different criteria. For further details or questions regarding the evaluation, please refer to the official publication or contact the organizers.\nGraphical representation Table ","categories":["Examples"],"description":"","excerpt":" Below you will find the official results for Task ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/results/task_a_results/","tags":"","title":"Task A Results"},{"body":" Below you will find the official results for Task B of TalentCLEF. This section provides both a graphical overview and a detailed table of the full results. The interactive visualizations are designed to help you explore and analyze the performance of participating systems based on different criteria. For further details or questions regarding the evaluation, please refer to the official publication or contact the organizers.\nGraphical representation Table ","categories":["Examples"],"description":"","excerpt":" Below you will find the official results for Task ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/results/task_b_results/","tags":"","title":"Task B Results"},{"body":" The schedule for the TalentCLEF Workshop at CLEF 2025 is now available! You can view the TalentCLEF full program here.\nFor details about the overall CLEF 2025 schedule, please check the official programme here.\nDon‚Äôt forget to register for the conference to secure your participation.\nWe look forward to seeing you at CLEF 2025! Status Date Event Link ‚úî 9th September 2024 Website release ‚úî 25th October 2024 Sample set release Link ‚úî 13th November 2024 Registration opens Link ‚úî 20th January 2025 Training data available for Tasks A and B Link ‚úî 17th February 2025 Start of Task A with the release of the development data Link ‚úî 17th March 2025 Start of Task B with the release of the development data Link ‚úî 21st April 2025 Task A Test set release Link ‚úî 21st April - 5th10th May 2025 Evaluation period of Task A Codabench Task A ‚úî 28st April 2025 Task B Test set release Link ‚úî 28st April - 5th10th May 2025 Evaluation period of Task B Codabench Task B ‚úî 7th May 202512th May 2025 Publication of Official Results Task A and Task B ‚úî 30th May 20254th June 2025 Submission of CLEF 2025 Working Notes ‚úî 4th June 2025 - 27th June Review of Working Notes ‚úî 27th June 2025 Notification of Acceptance for Participant Papers 9-12 September 2025 CLEF 2025 - Madrid Link ","categories":"","description":"Schedule and task deadlines\n","excerpt":"Schedule and task deadlines\n","ref":"/talentclef/docs/talentclef-2025/schedule/","tags":"","title":"Schedule"},{"body":" Info. The development set for Task B of TalentCLEF 2026 are now available! You can download the datasets from here. TalentCLEF 2026 schedule Status Date Event Link ‚úî 17th November 2025 Sample set release Link ‚úî 17th November 2025 Registration opens Link ‚úî 26th January 2026 2nd February 2026 Development Data Release for Task A Link ‚úî 2nd February 2026 Training Data Release for Task B Link ‚úî 16th February 2026 24th February 2026 Development Data Release for Task B Link ‚úî 2nd March 2026 Codabench Release for Task A Link 9th March 2026 Codabench Release for Task B 13th April - 3rd May 2026 Evaluation period of Task A 13th April - 3rd May 2026 Evaluation period of Task B 11th May 2025 Send Task A and B Results to participants 28th May 2026 Submission of CLEF 2026 Working Notes (Tentative) June - July 2025 Review of Labs Overviews (Tentative) ","categories":"","description":"Schedule and task deadlines\n","excerpt":"Schedule and task deadlines\n","ref":"/talentclef/docs/talentclef-2026/schedule/","tags":"","title":"Schedule"},{"body":" The evaluation will take place on Codabench, where participants must upload their predictions according to the submission format. The official competition links will be shared soon through this platform and online communication channels.\nTask A - Codabench rules For Task A, predictions for each language pair must be generated in separate run files. These files should be compressed into a ZIP file, ensuring that the files are placed directly inside the root of the archive (without any folders), and then uploaded to Codabench. During the development phase, participants can submit as many files as they wish in the platform. During the test phase, a maximum of 5 submissions can be uploaded. Each submission is required to contain at least the results for en-en and es-es; otherwise, an evaluation error will appear. We also suggest including cross-lingual predictions (en-es). Uploading rules:\nFile Naming Rules: Run files must follow the naming convention run_xx-xx_teamname.trec, where xx-xx represents the directionality of the queries and corpus elements. ZIP Compression: The Run files should be compressed into a flat ZIP file (i.e., the files must be placed directly inside the ZIP, without folders). Task B - Codabench rules During the development phase, participants can submit as many files as they wish in the platform. During the test phase, a maximum of 3 submissions can be uploaded. For local testing, an evaluation script is available in our GitHub repository. You can access it here: Evaluation Script. You can find a tutorial on the process of generating submission files and evaluating them in the Tutorials section of the Additional resources page\n","categories":["Examples"],"description":"","excerpt":" The evaluation will take place on Codabench, ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/evaluation/codabench/","tags":"","title":"Codabench"},{"body":" The evaluation will take place on Codabench, where participants must upload their predictions according to the submission format. The official competition links will be shared soon through this platform and online communication channels.\nTask A - Codabench rules For Task A, predictions for each language pair must be generated in separate .trec files. These files should be compressed into a ZIP file, ensuring that the files are placed directly inside the archive (without any folders), and then uploaded to Codabench. During the development phase, participants can submit as many files as they wish in the platform. During the test phase, a maximum of 6 submissions can be uploaded. Each submission is required to contain at least the results for en-en, es-es and de-de; otherwise, an evaluation error will appear. We also suggest including cross-lingual predictions (en-es, en-de), as well as the Chinese results (zh-zh, en-zh). Uploading rules:\nFile Naming Rules: TREC files must follow the naming convention run_xx-xx_teamname.trec, where xx-xx represents the directionality of the queries and corpus elements. ZIP Compression: The .trec files should be compressed into a flat ZIP file (i.e., the .trec files must be placed directly inside the ZIP, without folders). Task B - Codabench rules During the development phase, participants can submit as many files as they wish in the platform. During the test phase, a maximum of 3 submissions can be uploaded. For local testing, an evaluation script is available in our GitHub repository. You can access it here: Evaluation Script. You can find a tutorial on the process of generating submission files and evaluating them in the Tutorials section of the Additional resources page\nCodabench Tutorial To Be Published\n","categories":["Examples"],"description":"","excerpt":" The evaluation will take place on Codabench, ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/evaluation/codabench/","tags":"","title":"Codabench"},{"body":"","categories":"","description":"","excerpt":"","ref":"/talentclef/docs/talentclef-2025/data/","tags":"","title":"Data"},{"body":"","categories":"","description":"","excerpt":"","ref":"/talentclef/docs/talentclef-2026/data/","tags":"","title":"Data"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/talentclef/docs/talentclef-2025/evaluation/","tags":"","title":"Evaluation"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/talentclef/docs/talentclef-2026/evaluation/","tags":"","title":"Evaluation"},{"body":"Here you can find the main outcomes and resources from our project.\nDive into the official results for Task A and Task B, or explore our list of related publications for deeper insights.\nüìö Publications üÖ∞Ô∏è Results Task A üÖ±Ô∏è Results Task B ","categories":"","description":"","excerpt":"Here you can find the main outcomes and resources ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/results/","tags":"","title":"Results"},{"body":"The results will be published on this page once the task is completed. If you‚Äôd like to see last year‚Äôs results, you can find them here.\n","categories":"","description":"","excerpt":"The results will be published on this page once ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/results/","tags":"","title":"Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/talentclef/docs/talentclef-2025/people/","tags":"","title":"People"},{"body":"","categories":"","description":"","excerpt":"","ref":"/talentclef/docs/talentclef-2026/people/","tags":"","title":"People"},{"body":"If you have any concerns related to data or tasks, please don‚Äôt hesitate to reach out to us. You can contact us via email by replacing [at] with @ and [dot] with . in the following addresses:\nEmail:\nluis[dot]gasco[at]avature[dot]net hermenegildo[dot]fabregat[at]avature[dot]net ","categories":"","description":"","excerpt":"If you have any concerns related to data or tasks, ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/contact/","tags":"","title":"Contact"},{"body":"If you have any concerns related to data or tasks, please don‚Äôt hesitate to reach out to us. You can contact us via email by replacing [at] with @ and [dot] with . in the following addresses:\nEmail:\nluis[dot]gasco[at]avature[dot]net hermenegildo[dot]fabregat[at]avature[dot]net ","categories":"","description":"","excerpt":"If you have any concerns related to data or tasks, ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/contact/","tags":"","title":"Contact"},{"body":" TalentCLEF Workshop @ CLEF 2025 TalentCLEF 2025 workshop will be held as part of the CLEF 2025 conference (Conference and Labs of the Evaluation Forum), scheduled for September in Madrid, Spain.\nImportant Links View Schedule View Publications About the Workshop The TalentCLEF evaluation Lab will be a one-day event that will include several activities. There will be oral presentations of the best challenge solutions, keynote talks, a poster session for participants and a panel discussion. In order to boost participation, an awards ceremony will be held where diplomas will be awarded to the best performing teams. Below is a tentative schedule of the workshop activities.\nAbout the Workshop The TalentCLEF evaluation Lab will be a one-day event that will include several activities. There will be oral presentations of the best challenge solutions, keynote talks, a poster session for participants and a panel discussion. In order to boost participation, an awards ceremony will be held where diplomas will be awarded to the best performing teams. Below is a tentative schedule of the workshop activities.\nCLEF focuses on evaluating the effectiveness of information retrieval systems, such as search engines, text and multimedia retrieval systems, and other types of digital information systems.\nParticipants in the TalentCLEF workshop are required to submit a short paper detailing the systems they have developed for the task. Accepted submissions will be published in the CLEF 2025 CEUR proceedings.\nAt least one author from each accepted paper must register for the CLEF conference and present their system in a poster format. Additionally, outstanding participants, selected by the program committee, will have the opportunity to deliver an oral presentation of their work.\n","categories":"","description":"","excerpt":" TalentCLEF Workshop @ CLEF 2025 TalentCLEF 2025 ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/workshop/","tags":"","title":"Workshop"},{"body":" TalentCLEF Workshop @ CLEF 2026 TalentCLEF 2026 workshop will be held as part of the CLEF 2026 conference (Conference and Labs of the Evaluation Forum), scheduled for September in Jena, Germany.\nAbout the Workshop The TalentCLEF evaluation Lab will be a one-day event that will include several activities. There will be oral presentations of the best challenge solutions, keynote talks, a poster session for participants and a panel discussion. In order to boost participation, an awards ceremony will be held where diplomas will be awarded to the best performing teams. Below is a tentative schedule of the workshop activities.\nAbout the Workshop The TalentCLEF evaluation Lab will be a one-day event that will include several activities. There will be oral presentations of the best challenge solutions, keynote talks, a poster session for participants and a panel discussion. In order to boost participation, an awards ceremony will be held where diplomas will be awarded to the best performing teams. Below is a tentative schedule of the workshop activities.\nCLEF focuses on evaluating the effectiveness of information retrieval systems, such as search engines, text and multimedia retrieval systems, and other types of digital information systems.\nParticipants in the TalentCLEF workshop are required to submit a short paper detailing the systems they have developed for the task. Accepted submissions will be published in the CLEF 2026 CEUR proceedings.\nAt least one author from each accepted paper must register for the CLEF conference and present their system in a poster format. Additionally, outstanding participants, selected by the program committee, will have the opportunity to deliver an oral presentation of their work.\n","categories":"","description":"","excerpt":" TalentCLEF Workshop @ CLEF 2026 TalentCLEF 2026 ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/workshop/","tags":"","title":"Workshop"},{"body":" Si no eres redirigido autom√°ticamente, haz clic en este enlace.\n","categories":"","description":"","excerpt":" Si no eres redirigido autom√°ticamente, haz clic ‚Ä¶","ref":"/talentclef/docs/talentclef-2026/","tags":"","title":""},{"body":" Si no eres redirigido autom√°ticamente, haz clic en este enlace.\n","categories":"","description":"","excerpt":" Si no eres redirigido autom√°ticamente, haz clic ‚Ä¶","ref":"/talentclef/docs/talentclef-2025/","tags":"","title":""},{"body":"After a year of hard work and great participation, I‚Äôm excited to share that the first TalentCLEF Workshop will be held at #CLEF2025 in Madrid on September 11‚Äì12, 2025\nTalentCLEF has been an incredible community effort to push forward research on multilingual HR‚ÄìNLP solutions for skill and job title intelligence, tackling real challenges in talent management.\nThis year‚Äôs program includes 15 talks from the teams that took part in the evaluation campaign, showcasing innovative approaches to the tasks we set.\nWe are also honored to host a keynote by Marios Constantinides (CYENS Centre of Excellence), a top researcher in Human‚ÄìComputer Interaction, who will discuss how, in a world where AI is rapidly transforming the workplace, emerging technologies can enhance professional collaboration ‚Äî not only by boosting productivity, but also by supporting emotional awareness, psychological safety, and physical comfort in hybrid work environments.\nüëâ You can find the full workshop program and other relevant information here.\nLooking forward to having good discussions and meeting the community in Madrid! üá™üá∏\nJoin us!\nüí° If you are not able to attend in person, don‚Äôt worry ‚Äî you can still explore the results page to access the task overview and discover the publications generated by the participating teams.\n","categories":"","description":"","excerpt":"After a year of hard work and great participation, ‚Ä¶","ref":"/talentclef/blog/2025/09/05/announcing-the-first-talentclef-workshop-@-clef-2025/","tags":"","title":"Announcing the First TalentCLEF Workshop @ CLEF 2025"},{"body":" We are thrilled to announce the release of the sample set for the TalentCLEF 2025! This sample dataset serves as an essential resource for participants to familiarize themselves with the format, scope, and types of data they will encounter in the TalentCLEF tasks.\nThe sample set provides a practical preview, allowing participants to explore the dataset structure and begin preliminary analyses in preparation for the workshop. The release includes detailed documentation including a Corpus Description and the Dataset Format\nYou can download the sample set in Zenodo clicking the button below:\nAccess the Zenodo download page We encourage participants to review these resources closely, as they provide critical context for tackling the workshop‚Äôs upcoming challenges. For any questions or further information, please reach out via the TalentCLEF website. We look forward to seeing the innovative ways participants will use this data to address TalentCLEF‚Äôs objectives!\n","categories":"","description":"","excerpt":" We are thrilled to announce the release of the ‚Ä¶","ref":"/talentclef/blog/2024/10/25/release-of-sample-set-for-talentclef-2025/","tags":"","title":"Release of Sample Set for TalentCLEF 2025"},{"body":"Last week, on September 12, we presented the TalentCLEF Task at the CLEF2024 conference in Grenoble. This session provided an interesting opportunity to showcase the objectives, structure and expected impact of the task, generating a great deal of interest among researchers and practitioners alike.\nThe presentation highlighted the various facets of the TalentCLEF Task, including the unique challenges it addresses, the dataset design, and the evaluation criteria that participants will navigate.\nWe look forward to further promoting the task to interested participants through our TalentCLEF website.\nWe will be publishing the sample set soon.\nTalentCLEF presention @ CLEF2024\n","categories":"","description":"","excerpt":"Last week, on September 12, we presented the ‚Ä¶","ref":"/talentclef/blog/2024/09/19/presentation-of-talentclef-task-at-clef2024-in-grenoble/","tags":"","title":"Presentation of TalentCLEF Task at CLEF2024 in Grenoble"},{"body":"We are excited to announce the launch of the new TalentCLEF workshop website!\nThe website is designed to be a comprehensive resource for participants and researchers interested in the upcoming TalentCLEF 2025 workshop, providing an easy access to essential information about the event, including detailed descriptions of the workshop‚Äôs goals, schedule, and registration process.\nYou can track all the details about the TalentCLEF task in the task item, including the schedule, summary of the tasks, and new information about the data and evaluation criteria will be published soon.\nTalentCLEF website screenshot\n","categories":"","description":"","excerpt":"We are excited to announce the launch of the new ‚Ä¶","ref":"/talentclef/blog/2024/08/29/release-of-talentclef-website/","tags":"","title":"Release of TalentCLEF website"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: ‚Ä¶","ref":"/talentclef/blog/","tags":"","title":"News"},{"body":" Links to TalentCLEF related resources:\nGithub Official Github page of TalentCLEF CLEF 2026 CLEF 2026 website TalentCLEF 2026 schedule TalentCLEF task calendar TalentCLEF 2026 Corpora TalentCLEF 2026 Corpora TalentCLEF 2026 Evaluation Script TalentCLEF 2026 Evaluation Script TalentCLEF 2026: Task A - Codabench TalentCLEF 2026 Task A CodaBench TalentCLEF 2026: Task B - Codabench TalentCLEF 2026 Task B CodaBench (To be Published) ","categories":"","description":"","excerpt":" Links to TalentCLEF related resources:\nGithub ‚Ä¶","ref":"/talentclef/quick_links/","tags":"","title":"Quick links"},{"body":"","categories":"","description":"","excerpt":"","ref":"/talentclef/search/","tags":"","title":"Search Results"},{"body":" Info. The development set for Task B of TalentCLEF 2026 are now available! You can download the datasets from here. TalentCLEF 2026 schedule Status Date Event Link ‚úî 17th November 2025 Sample set release Link ‚úî 17th November 2025 Registration opens Link ‚úî 26th January 2026 2nd February 2026 Development Data Release for Task A Link ‚úî 2nd February 2026 Training Data Release for Task B Link ‚úî 16th February 2026 24th February 2026 Development Data Release for Task B Link ‚úî 2nd March 2026 Codabench Release for Task A Link 9th March 2026 Codabench Release for Task B 13th April - 3rd May 2026 Evaluation period of Task A 13th April - 3rd May 2026 Evaluation period of Task B 11th May 2025 Send Task A and B Results to participants 28th May 2026 Submission of CLEF 2026 Working Notes (Tentative) June - July 2025 Review of Labs Overviews (Tentative) ","categories":"","description":"","excerpt":" Info. The development set for Task B of ‚Ä¶","ref":"/talentclef/docs/","tags":"","title":"The task"}]